{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "from random import choices\n",
    "import random\n",
    "\n",
    "\n",
    "import kerastuner as kt\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "            \n",
    "class CVTuner(kt.engine.tuner.Tuner):\n",
    "    def run_trial(self, trial, X, y, splits, batch_size=32, epochs=1,callbacks=None):\n",
    "        val_losses = []\n",
    "        for train_indices, test_indices in splits:\n",
    "            X_train, X_test = [x[train_indices] for x in X], [x[test_indices] for x in X]\n",
    "            y_train, y_test = [a[train_indices] for a in y], [a[test_indices] for a in y]\n",
    "            if len(X_train) < 2:\n",
    "                X_train = X_train[0]\n",
    "                X_test = X_test[0]\n",
    "            if len(y_train) < 2:\n",
    "                y_train = y_train[0]\n",
    "                y_test = y_test[0]\n",
    "            \n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            hist = model.fit(X_train,y_train,\n",
    "                      validation_data=(X_test,y_test),\n",
    "                      epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                      callbacks=callbacks)\n",
    "            \n",
    "            val_losses.append([hist.history[k][-1] for k in hist.history])\n",
    "        val_losses = np.asarray(val_losses)\n",
    "        self.oracle.update_trial(trial.trial_id, {k:np.mean(val_losses[:,i]) for i,k in enumerate(hist.history.keys())})\n",
    "        self.save_model(trial.trial_id, model)\n",
    "\n",
    "# From https://medium.com/@micwurm/using-tensorflow-lite-to-speed-up-predictions-a3954886eb98\n",
    "\n",
    "class LiteModel:\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, model_path):\n",
    "        return LiteModel(tf.lite.Interpreter(model_path=model_path))\n",
    "    \n",
    "    @classmethod\n",
    "    def from_keras_model(cls, kmodel):\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(kmodel)\n",
    "        tflite_model = converter.convert()\n",
    "        return LiteModel(tf.lite.Interpreter(model_content=tflite_model))\n",
    "    \n",
    "    def __init__(self, interpreter):\n",
    "        self.interpreter = interpreter\n",
    "        self.interpreter.allocate_tensors()\n",
    "        input_det = self.interpreter.get_input_details()[0]\n",
    "        output_det = self.interpreter.get_output_details()[0]\n",
    "        self.input_index = input_det[\"index\"]\n",
    "        self.output_index = output_det[\"index\"]\n",
    "        self.input_shape = input_det[\"shape\"]\n",
    "        self.output_shape = output_det[\"shape\"]\n",
    "        self.input_dtype = input_det[\"dtype\"]\n",
    "        self.output_dtype = output_det[\"dtype\"]\n",
    "        \n",
    "    def predict(self, inp):\n",
    "        inp = inp.astype(self.input_dtype)\n",
    "        count = inp.shape[0]\n",
    "        out = np.zeros((count, self.output_shape[1]), dtype=self.output_dtype)\n",
    "        for i in range(count):\n",
    "            self.interpreter.set_tensor(self.input_index, inp[i:i+1])\n",
    "            self.interpreter.invoke()\n",
    "            out[i] = self.interpreter.get_tensor(self.output_index)[0]\n",
    "        return out\n",
    "    \n",
    "    def predict_single(self, inp):\n",
    "        \"\"\" Like predict(), but only for a single record. The input data can be a Python list. \"\"\"\n",
    "        inp = np.array([inp], dtype=self.input_dtype)\n",
    "        self.interpreter.set_tensor(self.input_index, inp)\n",
    "        self.interpreter.invoke()\n",
    "        out = self.interpreter.get_tensor(self.output_index)\n",
    "        return out[0]\n",
    "\n",
    "import shutil\n",
    "def del_his(n, v):\n",
    "    try:\n",
    "        shutil.rmtree(f'models_v{v}/jane_street_{n}')\n",
    "        os.remove(f\"models_v{v}/best_hp_{n}.pkl\")\n",
    "        # os.remove(f\"models_v{v}/model_{n}_0.hdf5\")\n",
    "        os.remove(f\"models_v{v}/model_{n}_0_finetune.hdf5\")\n",
    "        # os.remove(f\"models_v{v}/model_{n}_1.hdf5\")\n",
    "        os.remove(f\"models_v{v}/model_{n}_1_finetune.hdf5\")\n",
    "        # os.remove(f\"models_v{v}/model_{n}_2.hdf5\")\n",
    "        os.remove(f\"models_v{v}/model_{n}_2_finetune.hdf5\")\n",
    "        # os.remove(f\"models_v{v}/model_{n}_3.hdf5\")\n",
    "        os.remove(f\"models_v{v}/model_{n}_3_finetune.hdf5\")\n",
    "        os.remove(f\"models_v{v}/model_{n}_4.hdf5\")\n",
    "        os.remove(f\"models_v{v}/model_{n}_4_finetune.hdf5\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "train = train.query('date > 85').reset_index(drop = True) \n",
    "train = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\n",
    "train = train.query('weight > 0').reset_index(drop = True)\n",
    "train.fillna(train.mean(),inplace=True)\n",
    "\n",
    "features = [c for c in train.columns if 'feature' in c] + [\"weight\"]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(train[features].values)\n",
    "y = train[['resp_1', 'resp_2', 'resp_3', 'resp_4']].values\n",
    "\n",
    "f_mean = np.mean(train[features[1:]].values,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp_4']\n",
    "X_resp = train[['resp_1', 'resp_2', 'resp_3', 'resp_4']].values\n",
    "y_resp = np.stack([(train['resp'] > 0.000001).astype('int')]).T\n",
    "dates = train['date'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "[features, resp_cols, f_mean, x_scaler, y_scaler, models_resp] = pickle.load(open(f\"models_v{v}/conf.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## v7\n",
    "\n",
    "def create_autoencoder(input_dim,output_dim,noise=0.05):\n",
    "    i = Input(input_dim)\n",
    "    encoded = BatchNormalization()(i)\n",
    "    encoded = GaussianNoise(noise)(encoded)\n",
    "    encoded = Dense(640,activation='relu')(encoded)\n",
    "    decoded = Dropout(0.2)(encoded)\n",
    "    decoded = Dense(input_dim,name='decoded')(decoded)\n",
    "    x = Dense(320,activation='relu')(decoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(output_dim,activation='sigmoid',name='label_output')(x)\n",
    "    \n",
    "    encoder = Model(inputs=i,outputs=encoded)\n",
    "    autoencoder = Model(inputs=i,outputs=[decoded,x])\n",
    "    \n",
    "    autoencoder.compile(optimizer=Adam(0.001),loss={'decoded':'mse','label_output':'mse'})\n",
    "    return autoencoder, encoder\n",
    "\n",
    "def create_model(hp,input_dim,output_dim,encoder):\n",
    "    inputs = Input(input_dim)\n",
    "    \n",
    "    x = encoder(inputs)\n",
    "    x = Concatenate()([x,inputs]) #use both raw and encoded features\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(hp.Float('init_dropout',0.0,0.5))(x)\n",
    "    \n",
    "    for i in range(hp.Int('num_layers',1,5)):\n",
    "        x = Dense(hp.Int(f'num_units_{i}',128,256))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Lambda(tf.keras.activations.swish)(x)\n",
    "        x = Dropout(hp.Float(f'dropout_{i}',0.0,0.5))(x)\n",
    "    x = Dense(output_dim,activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs,outputs=x)\n",
    "    model.compile(optimizer=Adam(hp.Float('lr',0.00001,0.1,default=0.001)),loss=BinaryCrossentropy(label_smoothing=hp.Float('label_smoothing',0.0,0.1)),metrics=\"mse\")\n",
    "    return model\n",
    "\n",
    "v = 7\n",
    "try:\n",
    "    os.mkdir(f\"models_v{v}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump([features, resp_cols, f_mean, scaler], open(f\"models_v{v}/conf.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_his(5, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "87/87 [==============================] - 4s 45ms/step - loss: 0.1392 - decoded_loss: 0.0406 - label_output_loss: 0.0986 - val_loss: 0.0657 - val_decoded_loss: 0.0493 - val_label_output_loss: 0.0164\n",
      "Epoch 2/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 0.0184 - decoded_loss: 0.0088 - label_output_loss: 0.0097 - val_loss: 0.0297 - val_decoded_loss: 0.0282 - val_label_output_loss: 0.0015\n",
      "Epoch 3/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 0.0062 - decoded_loss: 0.0044 - label_output_loss: 0.0018 - val_loss: 0.0117 - val_decoded_loss: 0.0110 - val_label_output_loss: 6.5261e-04\n",
      "Epoch 4/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 0.0035 - decoded_loss: 0.0027 - label_output_loss: 8.3244e-04 - val_loss: 0.0054 - val_decoded_loss: 0.0048 - val_label_output_loss: 5.7731e-04\n",
      "Epoch 5/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 0.0026 - decoded_loss: 0.0019 - label_output_loss: 6.3360e-04 - val_loss: 0.0029 - val_decoded_loss: 0.0023 - val_label_output_loss: 5.4141e-04\n",
      "Epoch 6/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 0.0021 - decoded_loss: 0.0015 - label_output_loss: 5.5499e-04 - val_loss: 0.0017 - val_decoded_loss: 0.0011 - val_label_output_loss: 5.4822e-04\n",
      "Epoch 7/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 0.0018 - decoded_loss: 0.0012 - label_output_loss: 5.1403e-04 - val_loss: 0.0011 - val_decoded_loss: 6.1777e-04 - val_label_output_loss: 5.0441e-04\n",
      "Epoch 8/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 0.0016 - decoded_loss: 0.0011 - label_output_loss: 4.9019e-04 - val_loss: 8.9273e-04 - val_decoded_loss: 3.9543e-04 - val_label_output_loss: 4.9729e-04\n",
      "Epoch 9/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 0.0014 - decoded_loss: 9.4609e-04 - label_output_loss: 4.7461e-04 - val_loss: 7.7558e-04 - val_decoded_loss: 2.7575e-04 - val_label_output_loss: 4.9984e-04\n",
      "Epoch 10/1000\n",
      "87/87 [==============================] - 3s 39ms/step - loss: 0.0013 - decoded_loss: 8.5724e-04 - label_output_loss: 4.6417e-04 - val_loss: 7.1551e-04 - val_decoded_loss: 2.3031e-04 - val_label_output_loss: 4.8519e-04\n",
      "Epoch 11/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 0.0012 - decoded_loss: 7.8440e-04 - label_output_loss: 4.5600e-04 - val_loss: 6.6253e-04 - val_decoded_loss: 1.9131e-04 - val_label_output_loss: 4.7122e-04\n",
      "Epoch 12/1000\n",
      "87/87 [==============================] - 4s 40ms/step - loss: 0.0012 - decoded_loss: 7.2708e-04 - label_output_loss: 4.5018e-04 - val_loss: 6.4774e-04 - val_decoded_loss: 1.8322e-04 - val_label_output_loss: 4.6452e-04\n",
      "Epoch 13/1000\n",
      "87/87 [==============================] - 4s 43ms/step - loss: 0.0011 - decoded_loss: 6.8244e-04 - label_output_loss: 4.4593e-04 - val_loss: 6.1603e-04 - val_decoded_loss: 1.5508e-04 - val_label_output_loss: 4.6095e-04\n",
      "Epoch 14/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 0.0011 - decoded_loss: 6.4319e-04 - label_output_loss: 4.4173e-04 - val_loss: 6.0707e-04 - val_decoded_loss: 1.4735e-04 - val_label_output_loss: 4.5972e-04\n",
      "Epoch 15/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 0.0010 - decoded_loss: 6.1090e-04 - label_output_loss: 4.3892e-04 - val_loss: 5.8675e-04 - val_decoded_loss: 1.2904e-04 - val_label_output_loss: 4.5771e-04\n",
      "Epoch 16/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 0.0010 - decoded_loss: 5.8238e-04 - label_output_loss: 4.3671e-04 - val_loss: 5.8641e-04 - val_decoded_loss: 1.2950e-04 - val_label_output_loss: 4.5691e-04\n",
      "Epoch 17/1000\n",
      "87/87 [==============================] - 4s 40ms/step - loss: 9.9214e-04 - decoded_loss: 5.5740e-04 - label_output_loss: 4.3474e-04 - val_loss: 5.7263e-04 - val_decoded_loss: 1.1820e-04 - val_label_output_loss: 4.5443e-04\n",
      "Epoch 18/1000\n",
      "87/87 [==============================] - 4s 40ms/step - loss: 9.6969e-04 - decoded_loss: 5.3647e-04 - label_output_loss: 4.3322e-04 - val_loss: 5.6887e-04 - val_decoded_loss: 1.1404e-04 - val_label_output_loss: 4.5483e-04\n",
      "Epoch 19/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 9.4739e-04 - decoded_loss: 5.1552e-04 - label_output_loss: 4.3187e-04 - val_loss: 5.5670e-04 - val_decoded_loss: 1.0311e-04 - val_label_output_loss: 4.5358e-04\n",
      "Epoch 20/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 9.2888e-04 - decoded_loss: 4.9826e-04 - label_output_loss: 4.3062e-04 - val_loss: 5.6080e-04 - val_decoded_loss: 1.0760e-04 - val_label_output_loss: 4.5320e-04\n",
      "Epoch 21/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 9.1175e-04 - decoded_loss: 4.8197e-04 - label_output_loss: 4.2978e-04 - val_loss: 5.5346e-04 - val_decoded_loss: 9.9240e-05 - val_label_output_loss: 4.5422e-04\n",
      "Epoch 22/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 8.9632e-04 - decoded_loss: 4.6739e-04 - label_output_loss: 4.2893e-04 - val_loss: 5.4271e-04 - val_decoded_loss: 8.9573e-05 - val_label_output_loss: 4.5313e-04\n",
      "Epoch 23/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 8.8174e-04 - decoded_loss: 4.5353e-04 - label_output_loss: 4.2821e-04 - val_loss: 5.4491e-04 - val_decoded_loss: 9.0935e-05 - val_label_output_loss: 4.5397e-04\n",
      "Epoch 24/1000\n",
      "87/87 [==============================] - 3s 39ms/step - loss: 8.6787e-04 - decoded_loss: 4.4038e-04 - label_output_loss: 4.2749e-04 - val_loss: 5.4882e-04 - val_decoded_loss: 9.7017e-05 - val_label_output_loss: 4.5180e-04\n",
      "Epoch 25/1000\n",
      "87/87 [==============================] - 4s 40ms/step - loss: 8.5530e-04 - decoded_loss: 4.2837e-04 - label_output_loss: 4.2692e-04 - val_loss: 5.4213e-04 - val_decoded_loss: 9.1025e-05 - val_label_output_loss: 4.5110e-04\n",
      "Epoch 26/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 8.4287e-04 - decoded_loss: 4.1651e-04 - label_output_loss: 4.2636e-04 - val_loss: 5.3169e-04 - val_decoded_loss: 7.9986e-05 - val_label_output_loss: 4.5171e-04\n",
      "Epoch 27/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 8.3141e-04 - decoded_loss: 4.0548e-04 - label_output_loss: 4.2593e-04 - val_loss: 5.4436e-04 - val_decoded_loss: 9.3077e-05 - val_label_output_loss: 4.5128e-04\n",
      "Epoch 28/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 8.2084e-04 - decoded_loss: 3.9537e-04 - label_output_loss: 4.2547e-04 - val_loss: 5.3065e-04 - val_decoded_loss: 8.0029e-05 - val_label_output_loss: 4.5062e-04\n",
      "Epoch 29/1000\n",
      "87/87 [==============================] - 4s 42ms/step - loss: 8.0904e-04 - decoded_loss: 3.8390e-04 - label_output_loss: 4.2514e-04 - val_loss: 5.3101e-04 - val_decoded_loss: 8.0562e-05 - val_label_output_loss: 4.5044e-04\n",
      "Epoch 30/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 7.9968e-04 - decoded_loss: 3.7488e-04 - label_output_loss: 4.2480e-04 - val_loss: 5.2614e-04 - val_decoded_loss: 7.5898e-05 - val_label_output_loss: 4.5025e-04\n",
      "Epoch 31/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 7.8953e-04 - decoded_loss: 3.6495e-04 - label_output_loss: 4.2458e-04 - val_loss: 5.2848e-04 - val_decoded_loss: 7.5970e-05 - val_label_output_loss: 4.5252e-04\n",
      "Epoch 32/1000\n",
      "87/87 [==============================] - 4s 40ms/step - loss: 7.7983e-04 - decoded_loss: 3.5562e-04 - label_output_loss: 4.2422e-04 - val_loss: 5.2191e-04 - val_decoded_loss: 7.1347e-05 - val_label_output_loss: 4.5056e-04\n",
      "Epoch 33/1000\n",
      "87/87 [==============================] - 4s 40ms/step - loss: 7.7033e-04 - decoded_loss: 3.4630e-04 - label_output_loss: 4.2403e-04 - val_loss: 5.2522e-04 - val_decoded_loss: 7.4871e-05 - val_label_output_loss: 4.5035e-04\n",
      "Epoch 34/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 7.6178e-04 - decoded_loss: 3.3796e-04 - label_output_loss: 4.2382e-04 - val_loss: 5.2485e-04 - val_decoded_loss: 7.4640e-05 - val_label_output_loss: 4.5021e-04\n",
      "Epoch 35/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 7.5186e-04 - decoded_loss: 3.2834e-04 - label_output_loss: 4.2352e-04 - val_loss: 5.1672e-04 - val_decoded_loss: 6.6793e-05 - val_label_output_loss: 4.4993e-04\n",
      "Epoch 36/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 4s 41ms/step - loss: 7.4375e-04 - decoded_loss: 3.2007e-04 - label_output_loss: 4.2368e-04 - val_loss: 5.2078e-04 - val_decoded_loss: 6.7842e-05 - val_label_output_loss: 4.5294e-04\n",
      "Epoch 37/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 7.3518e-04 - decoded_loss: 3.1206e-04 - label_output_loss: 4.2312e-04 - val_loss: 5.1708e-04 - val_decoded_loss: 6.7225e-05 - val_label_output_loss: 4.4985e-04\n",
      "Epoch 38/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 7.2637e-04 - decoded_loss: 3.0344e-04 - label_output_loss: 4.2293e-04 - val_loss: 5.1306e-04 - val_decoded_loss: 6.3335e-05 - val_label_output_loss: 4.4973e-04\n",
      "Epoch 39/1000\n",
      "87/87 [==============================] - 4s 40ms/step - loss: 7.1827e-04 - decoded_loss: 2.9548e-04 - label_output_loss: 4.2279e-04 - val_loss: 5.1324e-04 - val_decoded_loss: 6.3654e-05 - val_label_output_loss: 4.4958e-04\n",
      "Epoch 40/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 7.1506e-04 - decoded_loss: 2.9233e-04 - label_output_loss: 4.2272e-04 - val_loss: 5.3962e-04 - val_decoded_loss: 8.9873e-05 - val_label_output_loss: 4.4975e-04\n",
      "Epoch 41/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 7.1338e-04 - decoded_loss: 2.9079e-04 - label_output_loss: 4.2259e-04 - val_loss: 5.1415e-04 - val_decoded_loss: 6.0798e-05 - val_label_output_loss: 4.5335e-04\n",
      "Epoch 42/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 6.9756e-04 - decoded_loss: 2.7507e-04 - label_output_loss: 4.2249e-04 - val_loss: 5.1038e-04 - val_decoded_loss: 6.0666e-05 - val_label_output_loss: 4.4971e-04\n",
      "Epoch 43/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 6.8914e-04 - decoded_loss: 2.6687e-04 - label_output_loss: 4.2227e-04 - val_loss: 5.0705e-04 - val_decoded_loss: 5.7426e-05 - val_label_output_loss: 4.4962e-04\n",
      "Epoch 44/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 6.8084e-04 - decoded_loss: 2.5855e-04 - label_output_loss: 4.2229e-04 - val_loss: 5.0621e-04 - val_decoded_loss: 5.6762e-05 - val_label_output_loss: 4.4945e-04\n",
      "Epoch 45/1000\n",
      "87/87 [==============================] - 4s 40ms/step - loss: 6.7435e-04 - decoded_loss: 2.5215e-04 - label_output_loss: 4.2221e-04 - val_loss: 5.0399e-04 - val_decoded_loss: 5.4153e-05 - val_label_output_loss: 4.4983e-04\n",
      "Epoch 46/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 6.6750e-04 - decoded_loss: 2.4527e-04 - label_output_loss: 4.2223e-04 - val_loss: 5.0392e-04 - val_decoded_loss: 5.4076e-05 - val_label_output_loss: 4.4984e-04\n",
      "Epoch 47/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 6.6069e-04 - decoded_loss: 2.3862e-04 - label_output_loss: 4.2207e-04 - val_loss: 5.0284e-04 - val_decoded_loss: 5.2794e-05 - val_label_output_loss: 4.5004e-04\n",
      "Epoch 48/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 6.5410e-04 - decoded_loss: 2.3238e-04 - label_output_loss: 4.2172e-04 - val_loss: 5.0868e-04 - val_decoded_loss: 5.1259e-05 - val_label_output_loss: 4.5742e-04\n",
      "Epoch 49/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 6.4890e-04 - decoded_loss: 2.2703e-04 - label_output_loss: 4.2187e-04 - val_loss: 5.0185e-04 - val_decoded_loss: 5.1753e-05 - val_label_output_loss: 4.5009e-04\n",
      "Epoch 50/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 6.4293e-04 - decoded_loss: 2.2103e-04 - label_output_loss: 4.2191e-04 - val_loss: 4.9818e-04 - val_decoded_loss: 4.9251e-05 - val_label_output_loss: 4.4893e-04\n",
      "Epoch 51/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 6.3667e-04 - decoded_loss: 2.1521e-04 - label_output_loss: 4.2146e-04 - val_loss: 4.9671e-04 - val_decoded_loss: 4.7731e-05 - val_label_output_loss: 4.4898e-04\n",
      "Epoch 52/1000\n",
      "87/87 [==============================] - 4s 40ms/step - loss: 6.3137e-04 - decoded_loss: 2.0983e-04 - label_output_loss: 4.2155e-04 - val_loss: 4.9687e-04 - val_decoded_loss: 4.7355e-05 - val_label_output_loss: 4.4951e-04\n",
      "Epoch 53/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 6.2666e-04 - decoded_loss: 2.0499e-04 - label_output_loss: 4.2167e-04 - val_loss: 4.9860e-04 - val_decoded_loss: 4.9178e-05 - val_label_output_loss: 4.4942e-04\n",
      "Epoch 54/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 6.2144e-04 - decoded_loss: 2.0014e-04 - label_output_loss: 4.2130e-04 - val_loss: 4.9423e-04 - val_decoded_loss: 4.5049e-05 - val_label_output_loss: 4.4918e-04\n",
      "Epoch 55/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 6.1851e-04 - decoded_loss: 1.9695e-04 - label_output_loss: 4.2156e-04 - val_loss: 5.1405e-04 - val_decoded_loss: 4.5274e-05 - val_label_output_loss: 4.6877e-04\n",
      "Epoch 56/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 6.1408e-04 - decoded_loss: 1.9250e-04 - label_output_loss: 4.2158e-04 - val_loss: 4.9213e-04 - val_decoded_loss: 4.3220e-05 - val_label_output_loss: 4.4891e-04\n",
      "Epoch 57/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 6.1000e-04 - decoded_loss: 1.8875e-04 - label_output_loss: 4.2124e-04 - val_loss: 4.9501e-04 - val_decoded_loss: 4.4898e-05 - val_label_output_loss: 4.5012e-04\n",
      "Epoch 58/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 6.0696e-04 - decoded_loss: 1.8577e-04 - label_output_loss: 4.2119e-04 - val_loss: 4.9250e-04 - val_decoded_loss: 4.2842e-05 - val_label_output_loss: 4.4966e-04\n",
      "Epoch 59/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 6.0531e-04 - decoded_loss: 1.8358e-04 - label_output_loss: 4.2174e-04 - val_loss: 8.1738e-04 - val_decoded_loss: 4.2768e-05 - val_label_output_loss: 7.7461e-04\n",
      "Epoch 60/1000\n",
      "87/87 [==============================] - 4s 40ms/step - loss: 6.0149e-04 - decoded_loss: 1.7987e-04 - label_output_loss: 4.2162e-04 - val_loss: 4.9143e-04 - val_decoded_loss: 4.0980e-05 - val_label_output_loss: 4.5045e-04\n",
      "Epoch 61/1000\n",
      "87/87 [==============================] - 4s 40ms/step - loss: 5.9886e-04 - decoded_loss: 1.7746e-04 - label_output_loss: 4.2139e-04 - val_loss: 4.9210e-04 - val_decoded_loss: 4.2584e-05 - val_label_output_loss: 4.4952e-04\n",
      "Epoch 62/1000\n",
      "87/87 [==============================] - 4s 40ms/step - loss: 5.9641e-04 - decoded_loss: 1.7515e-04 - label_output_loss: 4.2125e-04 - val_loss: 4.9101e-04 - val_decoded_loss: 4.1210e-05 - val_label_output_loss: 4.4980e-04\n",
      "Epoch 63/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.9472e-04 - decoded_loss: 1.7355e-04 - label_output_loss: 4.2117e-04 - val_loss: 5.0684e-04 - val_decoded_loss: 4.1215e-05 - val_label_output_loss: 4.6562e-04\n",
      "Epoch 64/1000\n",
      "87/87 [==============================] - 4s 42ms/step - loss: 5.9405e-04 - decoded_loss: 1.7252e-04 - label_output_loss: 4.2153e-04 - val_loss: 4.8881e-04 - val_decoded_loss: 3.9856e-05 - val_label_output_loss: 4.4896e-04\n",
      "Epoch 65/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.9109e-04 - decoded_loss: 1.7016e-04 - label_output_loss: 4.2093e-04 - val_loss: 4.8782e-04 - val_decoded_loss: 3.8617e-05 - val_label_output_loss: 4.4920e-04\n",
      "Epoch 66/1000\n",
      "87/87 [==============================] - 4s 42ms/step - loss: 5.9085e-04 - decoded_loss: 1.6966e-04 - label_output_loss: 4.2119e-04 - val_loss: 4.8772e-04 - val_decoded_loss: 3.7798e-05 - val_label_output_loss: 4.4993e-04\n",
      "Epoch 67/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.8917e-04 - decoded_loss: 1.6816e-04 - label_output_loss: 4.2100e-04 - val_loss: 4.8809e-04 - val_decoded_loss: 3.8181e-05 - val_label_output_loss: 4.4991e-04\n",
      "Epoch 68/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.8813e-04 - decoded_loss: 1.6715e-04 - label_output_loss: 4.2098e-04 - val_loss: 4.8586e-04 - val_decoded_loss: 3.6955e-05 - val_label_output_loss: 4.4890e-04\n",
      "Epoch 69/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.8690e-04 - decoded_loss: 1.6601e-04 - label_output_loss: 4.2089e-04 - val_loss: 4.8796e-04 - val_decoded_loss: 3.9196e-05 - val_label_output_loss: 4.4877e-04\n",
      "Epoch 70/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.8648e-04 - decoded_loss: 1.6542e-04 - label_output_loss: 4.2106e-04 - val_loss: 4.8515e-04 - val_decoded_loss: 3.6294e-05 - val_label_output_loss: 4.4885e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/1000\n",
      "87/87 [==============================] - 4s 42ms/step - loss: 5.8593e-04 - decoded_loss: 1.6500e-04 - label_output_loss: 4.2093e-04 - val_loss: 4.8560e-04 - val_decoded_loss: 3.6180e-05 - val_label_output_loss: 4.4942e-04\n",
      "Epoch 72/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.8480e-04 - decoded_loss: 1.6377e-04 - label_output_loss: 4.2103e-04 - val_loss: 4.8705e-04 - val_decoded_loss: 3.6490e-05 - val_label_output_loss: 4.5056e-04\n",
      "Epoch 73/1000\n",
      "87/87 [==============================] - 4s 42ms/step - loss: 5.8463e-04 - decoded_loss: 1.6365e-04 - label_output_loss: 4.2098e-04 - val_loss: 4.8796e-04 - val_decoded_loss: 3.7735e-05 - val_label_output_loss: 4.5022e-04\n",
      "Epoch 74/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.8393e-04 - decoded_loss: 1.6304e-04 - label_output_loss: 4.2088e-04 - val_loss: 5.3345e-04 - val_decoded_loss: 3.8942e-05 - val_label_output_loss: 4.9451e-04\n",
      "Epoch 75/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.8297e-04 - decoded_loss: 1.6213e-04 - label_output_loss: 4.2084e-04 - val_loss: 4.8577e-04 - val_decoded_loss: 3.6048e-05 - val_label_output_loss: 4.4972e-04\n",
      "Epoch 76/1000\n",
      "87/87 [==============================] - 4s 42ms/step - loss: 5.8339e-04 - decoded_loss: 1.6235e-04 - label_output_loss: 4.2103e-04 - val_loss: 4.8495e-04 - val_decoded_loss: 3.5551e-05 - val_label_output_loss: 4.4940e-04\n",
      "Epoch 77/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.8261e-04 - decoded_loss: 1.6164e-04 - label_output_loss: 4.2097e-04 - val_loss: 4.8376e-04 - val_decoded_loss: 3.3912e-05 - val_label_output_loss: 4.4985e-04\n",
      "Epoch 78/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.8237e-04 - decoded_loss: 1.6112e-04 - label_output_loss: 4.2125e-04 - val_loss: 4.8437e-04 - val_decoded_loss: 3.5102e-05 - val_label_output_loss: 4.4926e-04\n",
      "Epoch 79/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.8209e-04 - decoded_loss: 1.6085e-04 - label_output_loss: 4.2124e-04 - val_loss: 6.2253e-04 - val_decoded_loss: 3.5360e-05 - val_label_output_loss: 5.8717e-04\n",
      "Epoch 80/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.8171e-04 - decoded_loss: 1.6035e-04 - label_output_loss: 4.2137e-04 - val_loss: 4.8534e-04 - val_decoded_loss: 3.4779e-05 - val_label_output_loss: 4.5057e-04\n",
      "Epoch 81/1000\n",
      "87/87 [==============================] - 4s 40ms/step - loss: 5.8032e-04 - decoded_loss: 1.5915e-04 - label_output_loss: 4.2117e-04 - val_loss: 6.1085e-04 - val_decoded_loss: 3.3107e-05 - val_label_output_loss: 5.7774e-04\n",
      "Epoch 82/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.7999e-04 - decoded_loss: 1.5902e-04 - label_output_loss: 4.2097e-04 - val_loss: 0.0010 - val_decoded_loss: 3.5894e-05 - val_label_output_loss: 9.6857e-04\n",
      "Epoch 83/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.7973e-04 - decoded_loss: 1.5860e-04 - label_output_loss: 4.2113e-04 - val_loss: 4.8303e-04 - val_decoded_loss: 3.3517e-05 - val_label_output_loss: 4.4951e-04\n",
      "Epoch 84/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 5.7872e-04 - decoded_loss: 1.5780e-04 - label_output_loss: 4.2092e-04 - val_loss: 4.8522e-04 - val_decoded_loss: 3.4782e-05 - val_label_output_loss: 4.5044e-04\n",
      "Epoch 85/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 5.7987e-04 - decoded_loss: 1.5860e-04 - label_output_loss: 4.2127e-04 - val_loss: 4.8402e-04 - val_decoded_loss: 3.3786e-05 - val_label_output_loss: 4.5023e-04\n",
      "Epoch 86/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.7912e-04 - decoded_loss: 1.5766e-04 - label_output_loss: 4.2145e-04 - val_loss: 4.9356e-04 - val_decoded_loss: 3.3347e-05 - val_label_output_loss: 4.6021e-04\n",
      "Epoch 87/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 5.7721e-04 - decoded_loss: 1.5591e-04 - label_output_loss: 4.2130e-04 - val_loss: 4.8472e-04 - val_decoded_loss: 3.4169e-05 - val_label_output_loss: 4.5055e-04\n",
      "Epoch 88/1000\n",
      "87/87 [==============================] - 4s 40ms/step - loss: 5.7644e-04 - decoded_loss: 1.5544e-04 - label_output_loss: 4.2100e-04 - val_loss: 5.4408e-04 - val_decoded_loss: 3.5346e-05 - val_label_output_loss: 5.0873e-04\n",
      "Epoch 89/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.7594e-04 - decoded_loss: 1.5488e-04 - label_output_loss: 4.2106e-04 - val_loss: 4.8054e-04 - val_decoded_loss: 3.2248e-05 - val_label_output_loss: 4.4829e-04\n",
      "Epoch 90/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.7602e-04 - decoded_loss: 1.5521e-04 - label_output_loss: 4.2081e-04 - val_loss: 4.8160e-04 - val_decoded_loss: 3.2631e-05 - val_label_output_loss: 4.4897e-04\n",
      "Epoch 91/1000\n",
      "87/87 [==============================] - 3s 40ms/step - loss: 5.7703e-04 - decoded_loss: 1.5576e-04 - label_output_loss: 4.2127e-04 - val_loss: 4.8324e-04 - val_decoded_loss: 3.2566e-05 - val_label_output_loss: 4.5068e-04\n",
      "Epoch 92/1000\n",
      "87/87 [==============================] - 4s 40ms/step - loss: 5.7403e-04 - decoded_loss: 1.5300e-04 - label_output_loss: 4.2103e-04 - val_loss: 4.8459e-04 - val_decoded_loss: 3.4242e-05 - val_label_output_loss: 4.5035e-04\n",
      "Epoch 93/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.7553e-04 - decoded_loss: 1.5391e-04 - label_output_loss: 4.2162e-04 - val_loss: 4.8164e-04 - val_decoded_loss: 3.1148e-05 - val_label_output_loss: 4.5049e-04\n",
      "Epoch 94/1000\n",
      "87/87 [==============================] - 4s 42ms/step - loss: 5.7424e-04 - decoded_loss: 1.5290e-04 - label_output_loss: 4.2133e-04 - val_loss: 0.0022 - val_decoded_loss: 3.1186e-05 - val_label_output_loss: 0.0022\n",
      "Epoch 95/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.7583e-04 - decoded_loss: 1.5450e-04 - label_output_loss: 4.2134e-04 - val_loss: 4.8260e-04 - val_decoded_loss: 3.3134e-05 - val_label_output_loss: 4.4946e-04\n",
      "Epoch 96/1000\n",
      "87/87 [==============================] - 4s 42ms/step - loss: 5.7291e-04 - decoded_loss: 1.5191e-04 - label_output_loss: 4.2099e-04 - val_loss: 4.8043e-04 - val_decoded_loss: 3.1981e-05 - val_label_output_loss: 4.4845e-04\n",
      "Epoch 97/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.7275e-04 - decoded_loss: 1.5185e-04 - label_output_loss: 4.2089e-04 - val_loss: 8.8844e-04 - val_decoded_loss: 3.0891e-05 - val_label_output_loss: 8.5754e-04\n",
      "Epoch 98/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.7265e-04 - decoded_loss: 1.5166e-04 - label_output_loss: 4.2099e-04 - val_loss: 4.8021e-04 - val_decoded_loss: 2.9552e-05 - val_label_output_loss: 4.5066e-04\n",
      "Epoch 99/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.7247e-04 - decoded_loss: 1.5103e-04 - label_output_loss: 4.2144e-04 - val_loss: 4.8131e-04 - val_decoded_loss: 3.0799e-05 - val_label_output_loss: 4.5051e-04\n",
      "Epoch 100/1000\n",
      "87/87 [==============================] - 4s 42ms/step - loss: 5.7090e-04 - decoded_loss: 1.4983e-04 - label_output_loss: 4.2106e-04 - val_loss: 4.8384e-04 - val_decoded_loss: 3.3275e-05 - val_label_output_loss: 4.5056e-04\n",
      "Epoch 101/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.7189e-04 - decoded_loss: 1.5042e-04 - label_output_loss: 4.2147e-04 - val_loss: 4.7813e-04 - val_decoded_loss: 2.8657e-05 - val_label_output_loss: 4.4947e-04\n",
      "Epoch 102/1000\n",
      "87/87 [==============================] - 4s 40ms/step - loss: 5.6969e-04 - decoded_loss: 1.4873e-04 - label_output_loss: 4.2096e-04 - val_loss: 0.0026 - val_decoded_loss: 2.9504e-05 - val_label_output_loss: 0.0025\n",
      "Epoch 103/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.6983e-04 - decoded_loss: 1.4897e-04 - label_output_loss: 4.2086e-04 - val_loss: 4.7907e-04 - val_decoded_loss: 3.0219e-05 - val_label_output_loss: 4.4885e-04\n",
      "Epoch 104/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.6974e-04 - decoded_loss: 1.4877e-04 - label_output_loss: 4.2097e-04 - val_loss: 4.7897e-04 - val_decoded_loss: 3.0991e-05 - val_label_output_loss: 4.4798e-04\n",
      "Epoch 105/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.6867e-04 - decoded_loss: 1.4797e-04 - label_output_loss: 4.2071e-04 - val_loss: 4.8463e-04 - val_decoded_loss: 3.4309e-05 - val_label_output_loss: 4.5032e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.6893e-04 - decoded_loss: 1.4804e-04 - label_output_loss: 4.2089e-04 - val_loss: 4.7968e-04 - val_decoded_loss: 2.9957e-05 - val_label_output_loss: 4.4972e-04\n",
      "Epoch 107/1000\n",
      "87/87 [==============================] - 4s 42ms/step - loss: 5.7027e-04 - decoded_loss: 1.4921e-04 - label_output_loss: 4.2106e-04 - val_loss: 0.0188 - val_decoded_loss: 3.2148e-05 - val_label_output_loss: 0.0188\n",
      "Epoch 108/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.6832e-04 - decoded_loss: 1.4672e-04 - label_output_loss: 4.2160e-04 - val_loss: 0.0010 - val_decoded_loss: 2.9932e-05 - val_label_output_loss: 0.0010\n",
      "Epoch 109/1000\n",
      "87/87 [==============================] - 4s 41ms/step - loss: 5.6716e-04 - decoded_loss: 1.4571e-04 - label_output_loss: 4.2146e-04 - val_loss: 5.4337e-04 - val_decoded_loss: 3.0675e-05 - val_label_output_loss: 5.1269e-04\n",
      "Epoch 110/1000\n",
      "87/87 [==============================] - 4s 42ms/step - loss: 5.6672e-04 - decoded_loss: 1.4564e-04 - label_output_loss: 4.2109e-04 - val_loss: 4.7900e-04 - val_decoded_loss: 2.8448e-05 - val_label_output_loss: 4.5055e-04\n",
      "Epoch 111/1000\n",
      "87/87 [==============================] - 4s 43ms/step - loss: 5.6570e-04 - decoded_loss: 1.4491e-04 - label_output_loss: 4.2078e-04 - val_loss: 4.8344e-04 - val_decoded_loss: 3.2806e-05 - val_label_output_loss: 4.5064e-04\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "init_dropout      |0.31145           |?                 \n",
      "num_layers        |4                 |?                 \n",
      "num_units_0       |169               |?                 \n",
      "dropout_0         |0.11335           |?                 \n",
      "lr                |0.046306          |?                 \n",
      "label_smoothing   |0.05714           |?                 \n",
      "\n",
      "Epoch 1/300\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.2408 - mse: 0.0283WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 57ms/step - loss: 0.2376 - mse: 0.0275 - val_loss: 2.5894 - val_mse: 4.2675e-04\n",
      "Epoch 2/300\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1370 - mse: 0.0019WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.1368 - mse: 0.0019 - val_loss: 0.3726 - val_mse: 0.0063\n",
      "Epoch 3/300\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.1309 - mse: 0.0013WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.1308 - mse: 0.0013 - val_loss: 0.1723 - val_mse: 0.0028\n",
      "Epoch 4/300\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1303 - mse: 0.0012WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.1304 - mse: 0.0012 - val_loss: 0.1402 - val_mse: 0.0035\n",
      "Epoch 5/300\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1302 - mse: 0.0012WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 0.1302 - mse: 0.0012 - val_loss: 0.1479 - val_mse: 0.0055\n",
      "Epoch 6/300\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.1302 - mse: 0.0013WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 0.1302 - mse: 0.0013 - val_loss: 0.1467 - val_mse: 0.0052\n",
      "Epoch 7/300\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.1303 - mse: 0.0013WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 0.1303 - mse: 0.0013 - val_loss: 0.1425 - val_mse: 0.0042\n",
      "Epoch 8/300\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.1301 - mse: 0.0013- ETA: 0s - loss: 0.1301 - mse: 0.0WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.1302 - mse: 0.0012 - val_loss: 0.1378 - val_mse: 0.0032\n",
      "Epoch 9/300\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.1301 - mse: 0.0012WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.1301 - mse: 0.0012 - val_loss: 0.1353 - val_mse: 0.0027\n",
      "Epoch 10/300\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.1300 - mse: 0.0012WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.1300 - mse: 0.0012 - val_loss: 0.1334 - val_mse: 0.0022\n",
      "Epoch 11/300\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.1299 - mse: 0.0013WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 0.1299 - mse: 0.0013 - val_loss: 0.1319 - val_mse: 0.0018\n",
      "Epoch 12/300\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1299 - mse: 0.0013WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 0.1299 - mse: 0.0013 - val_loss: 0.1314 - val_mse: 0.0016\n",
      "Epoch 13/300\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.1295 - mse: 0.0012WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 0.1296 - mse: 0.0012 - val_loss: 0.1310 - val_mse: 0.0014\n",
      "Epoch 14/300\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1291 - mse: 0.0013WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.1290 - mse: 0.0013 - val_loss: 0.1311 - val_mse: 0.0011\n",
      "Epoch 15/300\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1281 - mse: 0.0013WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 0.1281 - mse: 0.0013 - val_loss: 0.1332 - val_mse: 7.6727e-04\n",
      "Epoch 16/300\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1237 - mse: 0.0013WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 0.1237 - mse: 0.0012 - val_loss: 0.1349 - val_mse: 6.8050e-04\n",
      "Epoch 17/300\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.1175 - mse: 0.0014WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.1172 - mse: 0.0014 - val_loss: 0.1433 - val_mse: 5.6144e-04\n",
      "Epoch 18/300\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.0961 - mse: 0.0014WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 0.0941 - mse: 0.0014 - val_loss: 0.1411 - val_mse: 5.6669e-04\n",
      "Epoch 19/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/18 [=========================>....] - ETA: 0s - loss: 0.0693 - mse: 0.0028WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 44ms/step - loss: 0.0640 - mse: 0.0026 - val_loss: 0.1816 - val_mse: 4.5053e-04\n",
      "Epoch 20/300\n",
      "17/18 [===========================>..] - ETA: 0s - loss: -0.0749 - mse: 0.0020WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 42ms/step - loss: -0.0783 - mse: 0.0019 - val_loss: 0.1498 - val_mse: 6.6030e-04\n",
      "Epoch 21/300\n",
      "16/18 [=========================>....] - ETA: 0s - loss: -0.3803 - mse: 0.0027WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 42ms/step - loss: -0.3809 - mse: 0.0026 - val_loss: 0.1597 - val_mse: 6.5517e-04\n",
      "Epoch 22/300\n",
      "16/18 [=========================>....] - ETA: 0s - loss: -0.7949 - mse: 0.0066WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 42ms/step - loss: -0.8486 - mse: 0.0097 - val_loss: 0.6766 - val_mse: 4.3323e-04\n",
      "Epoch 23/300\n",
      "17/18 [===========================>..] - ETA: 0s - loss: -1.5694 - mse: 0.0062WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 42ms/step - loss: -1.6561 - mse: 0.0061 - val_loss: 3.0672 - val_mse: 0.4715\n",
      "Epoch 24/300\n",
      "17/18 [===========================>..] - ETA: 0s - loss: -3.0118 - mse: 0.0074WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 42ms/step - loss: -3.0811 - mse: 0.0072 - val_loss: 6.9842 - val_mse: 0.5463\n",
      "Epoch 25/300\n",
      "17/18 [===========================>..] - ETA: 0s - loss: -4.1532 - mse: 0.0232WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 41ms/step - loss: -4.2153 - mse: 0.0224 - val_loss: 0.1647 - val_mse: 0.0823\n",
      "Epoch 26/300\n",
      "17/18 [===========================>..] - ETA: 0s - loss: -6.0631 - mse: 0.0195WARNING:tensorflow:Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: loss,mse,val_loss,val_mse\n",
      "18/18 [==============================] - 1s 45ms/step - loss: -6.1616 - mse: 0.0191 - val_loss: 0.7151 - val_mse: 0.0127\n",
      "Epoch 27/300\n",
      "17/18 [===========================>..] - ETA: 0s - loss: -7.8717 - mse: 0.0335"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\kerastuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-ef395aae50cf>\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, X, y, splits, batch_size, epochs, callbacks)\u001b[0m\n\u001b[0;32m    148\u001b[0m                       \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m                       callbacks=callbacks)\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[0mval_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m               return_dict=True)\n\u001b[0m\u001b[0;32m   1134\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'val_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TraceContext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    812\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "SEEDS = [5, 10, 15, 20]\n",
    "FOLDS = 5\n",
    "\n",
    "for j, SEED in enumerate(SEEDS):\n",
    "    set_all_seeds(SEED)\n",
    "\n",
    "    autoencoder, encoder = create_autoencoder(X.shape[-1],len(resp_cols),noise=0.1)\n",
    "    autoencoder.fit(X,(X,y),\n",
    "                    epochs=1000,\n",
    "                    batch_size=4096*4, \n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[EarlyStopping('val_loss',patience=10,restore_best_weights=True)])\n",
    "\n",
    "    encoder.trainable = False\n",
    "    model_fn = lambda hp: create_model(hp,X.shape[-1],len(resp_cols), encoder)\n",
    "\n",
    "    tuner = CVTuner(\n",
    "        hypermodel=model_fn,\n",
    "        oracle=kt.oracles.BayesianOptimization(\n",
    "        objective= kt.Objective('val_mse', direction='min'),\n",
    "        num_initial_points=4,\n",
    "        max_trials=20,\n",
    "        seed=SEED),\n",
    "        project_name=f'models_v{v}/jane_street_{SEED}'\n",
    "        )\n",
    "\n",
    "    gkf = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap=20)\n",
    "    splits = list(gkf.split(y, groups=train['date'].values))\n",
    "    tuner.search((X,),(y,),splits=splits,batch_size=4096*2,epochs=300,callbacks=[EarlyStopping('val_mse', mode='max',patience=5)])\n",
    "    hp  = tuner.get_best_hyperparameters(1)[0]\n",
    "    oof = np.zeros(y.shape)\n",
    "    pd.to_pickle(hp,f'models_v{v}/best_hp_{SEED}.pkl')\n",
    "    for fold, (train_indices, test_indices) in enumerate(splits):\n",
    "        model = model_fn(hp)\n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=300,batch_size=4096*2,callbacks=[EarlyStopping('val_mse',patience=10,restore_best_weights=True)])\n",
    "\n",
    "        # model.save_weights(f'./model_{SEED}_{fold}.hdf5')\n",
    "        model.compile(Adam(hp.get('lr')/100),loss='mse')\n",
    "        model.fit(X_test,y_test,epochs=6, batch_size=16384)\n",
    "        # encoder.save_weights(f'models_v{v}/encoder_{SEED}_{fold}.hdf5')\n",
    "        model.save_weights(f'models_v{v}/model_{SEED}_{fold}_finetune.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_df['weight'].item() > 0:\n",
    "    x_tt = test_df.loc[:, features].values\n",
    "    if np.isnan(x_tt[:, 1:].sum()):\n",
    "        x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n",
    "    x_tt = x_scaler.transform(x_tt)\n",
    "    pred = np.mean([model.predict(x_tt) for model in models],axis=0)\n",
    "#         pred = np.mean(pred)\n",
    "    pred = np.mean([model.predict(pred) for model in models_resp])\n",
    "    pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n",
    "else:\n",
    "    pred_df.action = 0\n",
    "env.predict(pred_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
