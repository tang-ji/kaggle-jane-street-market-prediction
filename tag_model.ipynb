{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5zFRrgDYdm77",
    "outputId": "e516dab4-df67-4a1b-8b19-eed0db39210a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/kaggle/JaneStreet\n"
     ]
    }
   ],
   "source": [
    "%cd drive/MyDrive/kaggle/JaneStreet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3VldfoqixiP",
    "outputId": "ff267b0c-0b3b-44e1-f55f-bc4cdfb05fc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/keras-team/keras-tuner\n",
    "!cd keras-tuner && pip -q install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7Mpo278VeWfT"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "from random import choices\n",
    "import random\n",
    "\n",
    "\n",
    "import kerastuner as kt\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HlewENnMPObE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]\n",
    "            \n",
    "class CVTuner(kt.engine.tuner.Tuner):\n",
    "    def run_trial(self, trial, X, y, splits, batch_size=32, epochs=1,callbacks=None):\n",
    "        val_losses = []\n",
    "        for train_indices, test_indices in splits:\n",
    "            X_train, X_test = [x[train_indices] for x in X], [x[test_indices] for x in X]\n",
    "            y_train, y_test = [a[train_indices] for a in y], [a[test_indices] for a in y]\n",
    "            if len(X_train) < 2:\n",
    "                X_train = X_train[0]\n",
    "                X_test = X_test[0]\n",
    "            if len(y_train) < 2:\n",
    "                y_train = y_train[0]\n",
    "                y_test = y_test[0]\n",
    "            \n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            hist = model.fit(X_train,y_train,\n",
    "                      validation_data=(X_test,y_test),\n",
    "                      epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                      callbacks=callbacks)\n",
    "            \n",
    "            val_losses.append([hist.history[k][-1] for k in hist.history])\n",
    "        val_losses = np.asarray(val_losses)\n",
    "        self.oracle.update_trial(trial.trial_id, {k:np.mean(val_losses[:,i]) for i,k in enumerate(hist.history.keys())})\n",
    "        self.save_model(trial.trial_id, model)\n",
    "\n",
    "# From https://medium.com/@micwurm/using-tensorflow-lite-to-speed-up-predictions-a3954886eb98\n",
    "\n",
    "class LiteModel:\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, model_path):\n",
    "        return LiteModel(tf.lite.Interpreter(model_path=model_path))\n",
    "    \n",
    "    @classmethod\n",
    "    def from_keras_model(cls, kmodel):\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(kmodel)\n",
    "        tflite_model = converter.convert()\n",
    "        return LiteModel(tf.lite.Interpreter(model_content=tflite_model))\n",
    "    \n",
    "    def __init__(self, interpreter):\n",
    "        self.interpreter = interpreter\n",
    "        self.interpreter.allocate_tensors()\n",
    "        input_det = self.interpreter.get_input_details()[0]\n",
    "        output_det = self.interpreter.get_output_details()[0]\n",
    "        self.input_index = input_det[\"index\"]\n",
    "        self.output_index = output_det[\"index\"]\n",
    "        self.input_shape = input_det[\"shape\"]\n",
    "        self.output_shape = output_det[\"shape\"]\n",
    "        self.input_dtype = input_det[\"dtype\"]\n",
    "        self.output_dtype = output_det[\"dtype\"]\n",
    "        \n",
    "    def predict(self, inp):\n",
    "        inp = inp.astype(self.input_dtype)\n",
    "        count = inp.shape[0]\n",
    "        out = np.zeros((count, self.output_shape[1]), dtype=self.output_dtype)\n",
    "        for i in range(count):\n",
    "            self.interpreter.set_tensor(self.input_index, inp[i:i+1])\n",
    "            self.interpreter.invoke()\n",
    "            out[i] = self.interpreter.get_tensor(self.output_index)[0]\n",
    "        return out\n",
    "    \n",
    "    def predict_single(self, inp):\n",
    "        \"\"\" Like predict(), but only for a single record. The input data can be a Python list. \"\"\"\n",
    "        inp = np.array([inp], dtype=self.input_dtype)\n",
    "        self.interpreter.set_tensor(self.input_index, inp)\n",
    "        self.interpreter.invoke()\n",
    "        out = self.interpreter.get_tensor(self.output_index)\n",
    "        return out[0]\n",
    "\n",
    "import shutil\n",
    "def del_his(n, v):\n",
    "    try:\n",
    "        shutil.rmtree(f'models_v{v}/jane_street_{n}')\n",
    "        os.remove(f\"models_v{v}/best_hp_{n}.pkl\")\n",
    "        # os.remove(f\"models_v{v}/model_{n}_0.hdf5\")\n",
    "        os.remove(f\"models_v{v}/model_{n}_0_finetune.hdf5\")\n",
    "        # os.remove(f\"models_v{v}/model_{n}_1.hdf5\")\n",
    "        os.remove(f\"models_v{v}/model_{n}_1_finetune.hdf5\")\n",
    "        # os.remove(f\"models_v{v}/model_{n}_2.hdf5\")\n",
    "        os.remove(f\"models_v{v}/model_{n}_2_finetune.hdf5\")\n",
    "        # os.remove(f\"models_v{v}/model_{n}_3.hdf5\")\n",
    "        os.remove(f\"models_v{v}/model_{n}_3_finetune.hdf5\")\n",
    "        os.remove(f\"models_v{v}/model_{n}_4.hdf5\")\n",
    "        os.remove(f\"models_v{v}/model_{n}_4_finetune.hdf5\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sh9HatiT5aGr"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "# train = train.query('date > 85').reset_index(drop = True) \n",
    "train = train[train['weight'] != 0]\n",
    "\n",
    "train.fillna(train.mean(),inplace=True)\n",
    "features = [c for c in train.columns if 'feature' in c]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "# embedding_matrix = pd.read_csv('features.csv').iloc[0:,1:].values.astype(np.float32)\n",
    "# X = train[features].values\n",
    "X = scaler.fit_transform(train[features].values)\n",
    "\n",
    "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n",
    "y = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T\n",
    "\n",
    "f_mean = np.mean(train[features[1:]].values,axis=0)\n",
    "\n",
    "feature = pd.read_csv('./features.csv')\n",
    "feature_list = [np.where(feature.values[:, i]==True)[0] for i in range(1, len(feature.values[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_ssp6kH87zjg"
   },
   "outputs": [],
   "source": [
    "def tag_layer(x):\n",
    "    x = BatchNormalization()(x)\n",
    "    # x = GaussianNoise(noise)(x)\n",
    "    x = Dense(32, activation=\"sigmoid\")(x)\n",
    "    x = Dropout(0.75)(x)\n",
    "    return x\n",
    "\n",
    "## v2\n",
    "def create_model(input_dim,output_dim):\n",
    "    inputs = Input(input_dim)\n",
    "    inputs2 = tf.keras.backend.square(inputs)\n",
    "    tags = []\n",
    "    for f in feature_list:\n",
    "        tags.append(tag_layer(tf.gather(inputs, tf.constant(f), axis=-1)))\n",
    "    Tags = Concatenate()(tags)\n",
    "    x = Concatenate(axis=-1)([inputs, inputs2, Tags])\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    for i in range(3):\n",
    "        x = Dense(256)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Lambda(tf.keras.activations.swish)(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(output_dim)(x)\n",
    "    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs,outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=tfa.optimizers.RectifiedAdam(1e-3),\n",
    "        loss=BinaryCrossentropy(label_smoothing=1e-3),\n",
    "        # loss=\"mse\",\n",
    "        metrics=tf.keras.metrics.AUC(name = 'auc')\n",
    "        )\n",
    "    return model\n",
    "v = 2\n",
    "try:\n",
    "    os.mkdir(f\"models_v{v}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_layer(x):\n",
    "    x = BatchNormalization()(x)\n",
    "    # x = GaussianNoise(noise)(x)\n",
    "    x = Dense(128, activation=\"sigmoid\")(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "    return x\n",
    "\n",
    "## v6\n",
    "def create_model(input_dim,output_dim):\n",
    "    inputs = Input(input_dim)\n",
    "    inputs2 = tf.keras.backend.square(inputs)\n",
    "    tags = []\n",
    "    for f in feature_list:\n",
    "        tags.append(tag_layer(tf.gather(inputs, tf.constant(f), axis=-1)))\n",
    "    Tags = Concatenate()(tags)\n",
    "    x = Concatenate(axis=-1)([inputs, inputs2, Tags])\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.8)(x)\n",
    "    for i in range(3):\n",
    "        x = Dense(256)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Lambda(tf.keras.activations.swish)(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(output_dim)(x)\n",
    "    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inputs,outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=tfa.optimizers.RectifiedAdam(1e-3),\n",
    "        loss=BinaryCrossentropy(label_smoothing=1e-3),\n",
    "        # loss=\"mse\",\n",
    "        metrics=tf.keras.metrics.AUC(name = 'auc')\n",
    "        )\n",
    "    return model\n",
    "v = 4\n",
    "try:\n",
    "    os.mkdir(f\"models_v{v}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BMQVZknKbg9s"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump([features, resp_cols, f_mean, scaler], open(f\"models_v{v}/conf.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fxJnyzJfOQPz"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_indices, test_indices = train_test_split(range(len(X)), test_size=0.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_ACswcD5aaU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ru04aBcOkatN",
    "outputId": "d1ecbd90-d13f-4484-a00e-a90a516a2266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "436/436 [==============================] - 56s 76ms/step - loss: 0.7855 - auc: 0.5005 - val_loss: 0.6926 - val_auc: 0.5247\n",
      "Epoch 2/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6976 - auc: 0.5073 - val_loss: 0.6911 - val_auc: 0.5320\n",
      "Epoch 3/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6921 - auc: 0.5221 - val_loss: 0.6905 - val_auc: 0.5364\n",
      "Epoch 4/500\n",
      "436/436 [==============================] - 31s 72ms/step - loss: 0.6915 - auc: 0.5278 - val_loss: 0.6901 - val_auc: 0.5395\n",
      "Epoch 5/500\n",
      "436/436 [==============================] - 31s 72ms/step - loss: 0.6911 - auc: 0.5316 - val_loss: 0.6899 - val_auc: 0.5405\n",
      "Epoch 6/500\n",
      "436/436 [==============================] - 31s 72ms/step - loss: 0.6909 - auc: 0.5338 - val_loss: 0.6895 - val_auc: 0.5436\n",
      "Epoch 7/500\n",
      "436/436 [==============================] - 31s 71ms/step - loss: 0.6908 - auc: 0.5345 - val_loss: 0.6895 - val_auc: 0.5435\n",
      "Epoch 8/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6907 - auc: 0.5355 - val_loss: 0.6892 - val_auc: 0.5454\n",
      "Epoch 9/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6905 - auc: 0.5365 - val_loss: 0.6891 - val_auc: 0.5460\n",
      "Epoch 10/500\n",
      "436/436 [==============================] - 31s 72ms/step - loss: 0.6905 - auc: 0.5369 - val_loss: 0.6891 - val_auc: 0.5467\n",
      "Epoch 11/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6904 - auc: 0.5378 - val_loss: 0.6891 - val_auc: 0.5463\n",
      "Epoch 12/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6903 - auc: 0.5386 - val_loss: 0.6889 - val_auc: 0.5479\n",
      "Epoch 13/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6903 - auc: 0.5383 - val_loss: 0.6887 - val_auc: 0.5491\n",
      "Epoch 14/500\n",
      "436/436 [==============================] - 31s 71ms/step - loss: 0.6901 - auc: 0.5400 - val_loss: 0.6887 - val_auc: 0.5497\n",
      "Epoch 15/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6901 - auc: 0.5393 - val_loss: 0.6886 - val_auc: 0.5499\n",
      "Epoch 16/500\n",
      "436/436 [==============================] - 31s 71ms/step - loss: 0.6899 - auc: 0.5408 - val_loss: 0.6887 - val_auc: 0.5490\n",
      "Epoch 17/500\n",
      "436/436 [==============================] - 31s 72ms/step - loss: 0.6899 - auc: 0.5411 - val_loss: 0.6885 - val_auc: 0.5504\n",
      "Epoch 18/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6899 - auc: 0.5408 - val_loss: 0.6886 - val_auc: 0.5505\n",
      "Epoch 19/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6897 - auc: 0.5422 - val_loss: 0.6884 - val_auc: 0.5517\n",
      "Epoch 20/500\n",
      "436/436 [==============================] - 31s 72ms/step - loss: 0.6897 - auc: 0.5426 - val_loss: 0.6884 - val_auc: 0.5513\n",
      "Epoch 21/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6896 - auc: 0.5424 - val_loss: 0.6882 - val_auc: 0.5520\n",
      "Epoch 22/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6896 - auc: 0.5433 - val_loss: 0.6881 - val_auc: 0.5524\n",
      "Epoch 23/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6895 - auc: 0.5434 - val_loss: 0.6882 - val_auc: 0.5528\n",
      "Epoch 24/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6894 - auc: 0.5438 - val_loss: 0.6880 - val_auc: 0.5525\n",
      "Epoch 25/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6894 - auc: 0.5442 - val_loss: 0.6881 - val_auc: 0.5536\n",
      "Epoch 26/500\n",
      "436/436 [==============================] - 31s 72ms/step - loss: 0.6893 - auc: 0.5446 - val_loss: 0.6880 - val_auc: 0.5529\n",
      "Epoch 27/500\n",
      "436/436 [==============================] - 32s 72ms/step - loss: 0.6891 - auc: 0.5452 - val_loss: 0.6879 - val_auc: 0.5538\n",
      "Epoch 28/500\n",
      "436/436 [==============================] - 31s 72ms/step - loss: 0.6891 - auc: 0.5454 - val_loss: 0.6879 - val_auc: 0.5538\n",
      "Epoch 29/500\n",
      "436/436 [==============================] - 31s 72ms/step - loss: 0.6889 - auc: 0.5469 - val_loss: 0.6878 - val_auc: 0.5537\n",
      "Epoch 30/500\n",
      "436/436 [==============================] - 31s 71ms/step - loss: 0.6890 - auc: 0.5466 - val_loss: 0.6877 - val_auc: 0.5537\n",
      "Epoch 31/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6890 - auc: 0.5463 - val_loss: 0.6876 - val_auc: 0.5544\n",
      "Epoch 32/500\n",
      "436/436 [==============================] - 31s 72ms/step - loss: 0.6889 - auc: 0.5470 - val_loss: 0.6876 - val_auc: 0.5546\n",
      "Epoch 33/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6887 - auc: 0.5474 - val_loss: 0.6876 - val_auc: 0.5548\n",
      "Epoch 34/500\n",
      "436/436 [==============================] - 31s 72ms/step - loss: 0.6888 - auc: 0.5475 - val_loss: 0.6876 - val_auc: 0.5548\n",
      "Epoch 35/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6887 - auc: 0.5479 - val_loss: 0.6875 - val_auc: 0.5552\n",
      "Epoch 36/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6887 - auc: 0.5477 - val_loss: 0.6874 - val_auc: 0.5552\n",
      "Epoch 37/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6885 - auc: 0.5484 - val_loss: 0.6874 - val_auc: 0.5551\n",
      "Epoch 38/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6886 - auc: 0.5488 - val_loss: 0.6876 - val_auc: 0.5535\n",
      "Epoch 39/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6885 - auc: 0.5487 - val_loss: 0.6876 - val_auc: 0.5544\n",
      "Epoch 40/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6884 - auc: 0.5494 - val_loss: 0.6873 - val_auc: 0.5556\n",
      "Epoch 41/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6885 - auc: 0.5492 - val_loss: 0.6874 - val_auc: 0.5554\n",
      "Epoch 42/500\n",
      "436/436 [==============================] - 31s 72ms/step - loss: 0.6885 - auc: 0.5490 - val_loss: 0.6872 - val_auc: 0.5561\n",
      "Epoch 43/500\n",
      "436/436 [==============================] - 31s 71ms/step - loss: 0.6884 - auc: 0.5489 - val_loss: 0.6874 - val_auc: 0.5561\n",
      "Epoch 44/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6884 - auc: 0.5491 - val_loss: 0.6873 - val_auc: 0.5553\n",
      "Epoch 45/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6884 - auc: 0.5495 - val_loss: 0.6873 - val_auc: 0.5558\n",
      "Epoch 46/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6883 - auc: 0.5501 - val_loss: 0.6871 - val_auc: 0.5563\n",
      "Epoch 47/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6882 - auc: 0.5505 - val_loss: 0.6873 - val_auc: 0.5557\n",
      "Epoch 48/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6882 - auc: 0.5501 - val_loss: 0.6874 - val_auc: 0.5551\n",
      "Epoch 49/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6882 - auc: 0.5504 - val_loss: 0.6872 - val_auc: 0.5557\n",
      "Epoch 50/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6881 - auc: 0.5511 - val_loss: 0.6872 - val_auc: 0.5561\n",
      "Epoch 51/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6882 - auc: 0.5504 - val_loss: 0.6871 - val_auc: 0.5564\n",
      "Epoch 52/500\n",
      "436/436 [==============================] - 32s 72ms/step - loss: 0.6881 - auc: 0.5507 - val_loss: 0.6871 - val_auc: 0.5562\n",
      "Epoch 53/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6881 - auc: 0.5509 - val_loss: 0.6873 - val_auc: 0.5558\n",
      "Epoch 54/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6880 - auc: 0.5510 - val_loss: 0.6872 - val_auc: 0.5558\n",
      "Epoch 55/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6880 - auc: 0.5515 - val_loss: 0.6870 - val_auc: 0.5561\n",
      "Epoch 56/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6880 - auc: 0.5515 - val_loss: 0.6870 - val_auc: 0.5560\n",
      "Epoch 57/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6880 - auc: 0.5513 - val_loss: 0.6869 - val_auc: 0.5568\n",
      "Epoch 58/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6879 - auc: 0.5519 - val_loss: 0.6870 - val_auc: 0.5563\n",
      "Epoch 59/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6878 - auc: 0.5522 - val_loss: 0.6871 - val_auc: 0.5556\n",
      "Epoch 60/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6879 - auc: 0.5520 - val_loss: 0.6870 - val_auc: 0.5559\n",
      "Epoch 61/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6878 - auc: 0.5521 - val_loss: 0.6869 - val_auc: 0.5560\n",
      "Epoch 62/500\n",
      "436/436 [==============================] - 31s 72ms/step - loss: 0.6877 - auc: 0.5526 - val_loss: 0.6869 - val_auc: 0.5561\n",
      "Epoch 63/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6877 - auc: 0.5524 - val_loss: 0.6870 - val_auc: 0.5559\n",
      "Epoch 64/500\n",
      "436/436 [==============================] - 32s 72ms/step - loss: 0.6877 - auc: 0.5528 - val_loss: 0.6870 - val_auc: 0.5557\n",
      "Epoch 65/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6876 - auc: 0.5530 - val_loss: 0.6870 - val_auc: 0.5566\n",
      "Epoch 66/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6877 - auc: 0.5532 - val_loss: 0.6870 - val_auc: 0.5556\n",
      "Epoch 67/500\n",
      "436/436 [==============================] - 31s 72ms/step - loss: 0.6876 - auc: 0.5529 - val_loss: 0.6870 - val_auc: 0.5557\n",
      "Epoch 68/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6876 - auc: 0.5531 - val_loss: 0.6871 - val_auc: 0.5554\n",
      "Epoch 69/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6875 - auc: 0.5535 - val_loss: 0.6869 - val_auc: 0.5563\n",
      "Epoch 70/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6875 - auc: 0.5533 - val_loss: 0.6868 - val_auc: 0.5566\n",
      "Epoch 71/500\n",
      "436/436 [==============================] - 32s 72ms/step - loss: 0.6876 - auc: 0.5531 - val_loss: 0.6868 - val_auc: 0.5567\n",
      "Epoch 72/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6875 - auc: 0.5533 - val_loss: 0.6868 - val_auc: 0.5565\n",
      "Epoch 73/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6874 - auc: 0.5538 - val_loss: 0.6868 - val_auc: 0.5562\n",
      "Epoch 74/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6874 - auc: 0.5541 - val_loss: 0.6869 - val_auc: 0.5561\n",
      "Epoch 75/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6874 - auc: 0.5541 - val_loss: 0.6869 - val_auc: 0.5560\n",
      "Epoch 76/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6874 - auc: 0.5536 - val_loss: 0.6870 - val_auc: 0.5559\n",
      "Epoch 77/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6874 - auc: 0.5538 - val_loss: 0.6870 - val_auc: 0.5560\n",
      "Epoch 78/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6873 - auc: 0.5546 - val_loss: 0.6867 - val_auc: 0.5566\n",
      "Epoch 79/500\n",
      "436/436 [==============================] - 31s 72ms/step - loss: 0.6873 - auc: 0.5540 - val_loss: 0.6869 - val_auc: 0.5570\n",
      "Epoch 80/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6874 - auc: 0.5538 - val_loss: 0.6867 - val_auc: 0.5567\n",
      "Epoch 81/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6872 - auc: 0.5551 - val_loss: 0.6869 - val_auc: 0.5564\n",
      "Epoch 82/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6872 - auc: 0.5548 - val_loss: 0.6869 - val_auc: 0.5560\n",
      "Epoch 83/500\n",
      "436/436 [==============================] - 32s 75ms/step - loss: 0.6872 - auc: 0.5544 - val_loss: 0.6869 - val_auc: 0.5561\n",
      "Epoch 84/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6872 - auc: 0.5546 - val_loss: 0.6868 - val_auc: 0.5566\n",
      "Epoch 85/500\n",
      "436/436 [==============================] - 31s 72ms/step - loss: 0.6872 - auc: 0.5549 - val_loss: 0.6869 - val_auc: 0.5559\n",
      "Epoch 86/500\n",
      "436/436 [==============================] - 31s 72ms/step - loss: 0.6872 - auc: 0.5549 - val_loss: 0.6869 - val_auc: 0.5557\n",
      "Epoch 87/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6872 - auc: 0.5549 - val_loss: 0.6869 - val_auc: 0.5557\n",
      "Epoch 88/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6870 - auc: 0.5553 - val_loss: 0.6868 - val_auc: 0.5562\n",
      "Epoch 89/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6870 - auc: 0.5557 - val_loss: 0.6869 - val_auc: 0.5559\n",
      "Epoch 90/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6871 - auc: 0.5552 - val_loss: 0.6868 - val_auc: 0.5564\n",
      "Epoch 91/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6870 - auc: 0.5555 - val_loss: 0.6868 - val_auc: 0.5562\n",
      "Epoch 92/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6870 - auc: 0.5555 - val_loss: 0.6869 - val_auc: 0.5556\n",
      "Epoch 93/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6871 - auc: 0.5551 - val_loss: 0.6868 - val_auc: 0.5559\n",
      "Epoch 94/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6868 - auc: 0.5563 - val_loss: 0.6869 - val_auc: 0.5562\n",
      "Epoch 95/500\n",
      "436/436 [==============================] - 33s 77ms/step - loss: 0.6871 - auc: 0.5555 - val_loss: 0.6869 - val_auc: 0.5559\n",
      "Epoch 96/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6870 - auc: 0.5558 - val_loss: 0.6868 - val_auc: 0.5563\n",
      "Epoch 97/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6869 - auc: 0.5555 - val_loss: 0.6869 - val_auc: 0.5558\n",
      "Epoch 98/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6869 - auc: 0.5557 - val_loss: 0.6870 - val_auc: 0.5554\n",
      "Epoch 99/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6868 - auc: 0.5559 - val_loss: 0.6870 - val_auc: 0.5560\n",
      "Epoch 100/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6869 - auc: 0.5555 - val_loss: 0.6870 - val_auc: 0.5561\n",
      "Epoch 101/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6868 - auc: 0.5562 - val_loss: 0.6869 - val_auc: 0.5558\n",
      "Epoch 102/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6868 - auc: 0.5566 - val_loss: 0.6868 - val_auc: 0.5561\n",
      "Epoch 103/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6868 - auc: 0.5564 - val_loss: 0.6869 - val_auc: 0.5563\n",
      "Epoch 104/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6868 - auc: 0.5564 - val_loss: 0.6869 - val_auc: 0.5559\n",
      "Epoch 105/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6867 - auc: 0.5564 - val_loss: 0.6871 - val_auc: 0.5551\n",
      "Epoch 106/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6867 - auc: 0.5568 - val_loss: 0.6868 - val_auc: 0.5561\n",
      "Epoch 107/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6866 - auc: 0.5570 - val_loss: 0.6868 - val_auc: 0.5561\n",
      "Epoch 108/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6866 - auc: 0.5569 - val_loss: 0.6868 - val_auc: 0.5564\n",
      "Epoch 109/500\n",
      "436/436 [==============================] - 32s 73ms/step - loss: 0.6867 - auc: 0.5562 - val_loss: 0.6867 - val_auc: 0.5568\n",
      "Epoch 110/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6868 - auc: 0.5562 - val_loss: 0.6868 - val_auc: 0.5561\n",
      "Epoch 111/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6866 - auc: 0.5567 - val_loss: 0.6868 - val_auc: 0.5565\n",
      "Epoch 112/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6866 - auc: 0.5565 - val_loss: 0.6869 - val_auc: 0.5564\n",
      "Epoch 113/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6866 - auc: 0.5567 - val_loss: 0.6869 - val_auc: 0.5558\n",
      "Epoch 114/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6866 - auc: 0.5569 - val_loss: 0.6869 - val_auc: 0.5555\n",
      "Epoch 115/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6867 - auc: 0.5570 - val_loss: 0.6868 - val_auc: 0.5563\n",
      "Epoch 116/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6865 - auc: 0.5575 - val_loss: 0.6869 - val_auc: 0.5560\n",
      "Epoch 117/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6865 - auc: 0.5572 - val_loss: 0.6868 - val_auc: 0.5564\n",
      "Epoch 118/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6866 - auc: 0.5569 - val_loss: 0.6869 - val_auc: 0.5561\n",
      "Epoch 119/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6865 - auc: 0.5575 - val_loss: 0.6868 - val_auc: 0.5564\n",
      "Epoch 120/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6865 - auc: 0.5577 - val_loss: 0.6869 - val_auc: 0.5562\n",
      "Epoch 121/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6866 - auc: 0.5569 - val_loss: 0.6869 - val_auc: 0.5562\n",
      "Epoch 122/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6864 - auc: 0.5570 - val_loss: 0.6868 - val_auc: 0.5566\n",
      "Epoch 123/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6866 - auc: 0.5567 - val_loss: 0.6869 - val_auc: 0.5563\n",
      "Epoch 124/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6864 - auc: 0.5573 - val_loss: 0.6868 - val_auc: 0.5563\n",
      "Epoch 125/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6865 - auc: 0.5569 - val_loss: 0.6867 - val_auc: 0.5565\n",
      "Epoch 126/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6864 - auc: 0.5572 - val_loss: 0.6869 - val_auc: 0.5563\n",
      "Epoch 127/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6863 - auc: 0.5585 - val_loss: 0.6868 - val_auc: 0.5569\n",
      "Epoch 128/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6864 - auc: 0.5575 - val_loss: 0.6868 - val_auc: 0.5563\n",
      "Epoch 129/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6864 - auc: 0.5572 - val_loss: 0.6869 - val_auc: 0.5557\n",
      "Epoch 130/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6864 - auc: 0.5579 - val_loss: 0.6868 - val_auc: 0.5562\n",
      "Epoch 131/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6863 - auc: 0.5580 - val_loss: 0.6870 - val_auc: 0.5560\n",
      "Epoch 132/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6862 - auc: 0.5585 - val_loss: 0.6870 - val_auc: 0.5556\n",
      "Epoch 133/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6864 - auc: 0.5577 - val_loss: 0.6869 - val_auc: 0.5565\n",
      "Epoch 134/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6863 - auc: 0.5578 - val_loss: 0.6869 - val_auc: 0.5559\n",
      "Epoch 135/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6863 - auc: 0.5578 - val_loss: 0.6869 - val_auc: 0.5558\n",
      "Epoch 136/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6863 - auc: 0.5578 - val_loss: 0.6869 - val_auc: 0.5564\n",
      "Epoch 137/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6861 - auc: 0.5589 - val_loss: 0.6868 - val_auc: 0.5561\n",
      "Epoch 138/500\n",
      "436/436 [==============================] - 33s 77ms/step - loss: 0.6863 - auc: 0.5578 - val_loss: 0.6870 - val_auc: 0.5564\n",
      "Epoch 139/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6863 - auc: 0.5578 - val_loss: 0.6869 - val_auc: 0.5561\n",
      "Epoch 140/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6863 - auc: 0.5581 - val_loss: 0.6868 - val_auc: 0.5567\n",
      "Epoch 141/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6862 - auc: 0.5581 - val_loss: 0.6869 - val_auc: 0.5560\n",
      "Epoch 142/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6860 - auc: 0.5590 - val_loss: 0.6869 - val_auc: 0.5562\n",
      "Epoch 143/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6860 - auc: 0.5590 - val_loss: 0.6869 - val_auc: 0.5559\n",
      "Epoch 144/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6861 - auc: 0.5587 - val_loss: 0.6869 - val_auc: 0.5563\n",
      "Epoch 145/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6860 - auc: 0.5589 - val_loss: 0.6869 - val_auc: 0.5566\n",
      "Epoch 146/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6861 - auc: 0.5586 - val_loss: 0.6869 - val_auc: 0.5562\n",
      "Epoch 147/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6861 - auc: 0.5587 - val_loss: 0.6869 - val_auc: 0.5561\n",
      "Epoch 148/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6860 - auc: 0.5587 - val_loss: 0.6869 - val_auc: 0.5561\n",
      "Epoch 149/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6861 - auc: 0.5580 - val_loss: 0.6868 - val_auc: 0.5564\n",
      "Epoch 150/500\n",
      "436/436 [==============================] - 33s 77ms/step - loss: 0.6859 - auc: 0.5594 - val_loss: 0.6870 - val_auc: 0.5562\n",
      "Epoch 151/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6860 - auc: 0.5587 - val_loss: 0.6869 - val_auc: 0.5563\n",
      "Epoch 152/500\n",
      "436/436 [==============================] - 33s 77ms/step - loss: 0.6860 - auc: 0.5586 - val_loss: 0.6867 - val_auc: 0.5568\n",
      "Epoch 153/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6860 - auc: 0.5587 - val_loss: 0.6869 - val_auc: 0.5562\n",
      "Epoch 154/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6860 - auc: 0.5588 - val_loss: 0.6869 - val_auc: 0.5563\n",
      "Epoch 155/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6860 - auc: 0.5588 - val_loss: 0.6868 - val_auc: 0.5564\n",
      "Epoch 156/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6860 - auc: 0.5590 - val_loss: 0.6869 - val_auc: 0.5560\n",
      "Epoch 157/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6859 - auc: 0.5588 - val_loss: 0.6868 - val_auc: 0.5568\n",
      "Epoch 158/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6859 - auc: 0.5587 - val_loss: 0.6868 - val_auc: 0.5567\n",
      "Epoch 159/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6859 - auc: 0.5590 - val_loss: 0.6870 - val_auc: 0.5561\n",
      "Epoch 160/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6859 - auc: 0.5595 - val_loss: 0.6869 - val_auc: 0.5560\n",
      "Epoch 161/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6858 - auc: 0.5596 - val_loss: 0.6868 - val_auc: 0.5560\n",
      "Epoch 162/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6859 - auc: 0.5588 - val_loss: 0.6869 - val_auc: 0.5564\n",
      "Epoch 163/500\n",
      "436/436 [==============================] - 33s 77ms/step - loss: 0.6859 - auc: 0.5587 - val_loss: 0.6868 - val_auc: 0.5566\n",
      "Epoch 164/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6858 - auc: 0.5594 - val_loss: 0.6869 - val_auc: 0.5562\n",
      "Epoch 165/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6858 - auc: 0.5592 - val_loss: 0.6869 - val_auc: 0.5564\n",
      "Epoch 166/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6859 - auc: 0.5590 - val_loss: 0.6870 - val_auc: 0.5556\n",
      "Epoch 167/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6859 - auc: 0.5589 - val_loss: 0.6870 - val_auc: 0.5564\n",
      "Epoch 168/500\n",
      "436/436 [==============================] - 33s 77ms/step - loss: 0.6858 - auc: 0.5593 - val_loss: 0.6869 - val_auc: 0.5559\n",
      "Epoch 169/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6858 - auc: 0.5593 - val_loss: 0.6870 - val_auc: 0.5557\n",
      "Epoch 170/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6858 - auc: 0.5594 - val_loss: 0.6869 - val_auc: 0.5564\n",
      "Epoch 171/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6857 - auc: 0.5593 - val_loss: 0.6870 - val_auc: 0.5560\n",
      "Epoch 172/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6858 - auc: 0.5594 - val_loss: 0.6870 - val_auc: 0.5557\n",
      "Epoch 173/500\n",
      "436/436 [==============================] - 33s 77ms/step - loss: 0.6857 - auc: 0.5600 - val_loss: 0.6869 - val_auc: 0.5561\n",
      "Epoch 174/500\n",
      "436/436 [==============================] - 33s 77ms/step - loss: 0.6859 - auc: 0.5591 - val_loss: 0.6869 - val_auc: 0.5565\n",
      "Epoch 175/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6857 - auc: 0.5596 - val_loss: 0.6869 - val_auc: 0.5564\n",
      "Epoch 176/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6857 - auc: 0.5596 - val_loss: 0.6869 - val_auc: 0.5561\n",
      "Epoch 177/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6858 - auc: 0.5593 - val_loss: 0.6870 - val_auc: 0.5556\n",
      "Epoch 178/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6857 - auc: 0.5601 - val_loss: 0.6868 - val_auc: 0.5567\n",
      "Epoch 179/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6856 - auc: 0.5603 - val_loss: 0.6870 - val_auc: 0.5563\n",
      "Epoch 180/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6859 - auc: 0.5588 - val_loss: 0.6869 - val_auc: 0.5568\n",
      "Epoch 181/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6856 - auc: 0.5601 - val_loss: 0.6870 - val_auc: 0.5557\n",
      "Epoch 182/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6856 - auc: 0.5598 - val_loss: 0.6870 - val_auc: 0.5561\n",
      "Epoch 183/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6856 - auc: 0.5597 - val_loss: 0.6870 - val_auc: 0.5562\n",
      "Epoch 184/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6857 - auc: 0.5599 - val_loss: 0.6871 - val_auc: 0.5559\n",
      "Epoch 185/500\n",
      "436/436 [==============================] - 34s 79ms/step - loss: 0.6857 - auc: 0.5595 - val_loss: 0.6869 - val_auc: 0.5564\n",
      "Epoch 186/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6856 - auc: 0.5600 - val_loss: 0.6870 - val_auc: 0.5559\n",
      "Epoch 187/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6856 - auc: 0.5602 - val_loss: 0.6870 - val_auc: 0.5560\n",
      "Epoch 188/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6856 - auc: 0.5598 - val_loss: 0.6869 - val_auc: 0.5562\n",
      "Epoch 189/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6856 - auc: 0.5599 - val_loss: 0.6870 - val_auc: 0.5567\n",
      "Epoch 190/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6855 - auc: 0.5609 - val_loss: 0.6869 - val_auc: 0.5561\n",
      "Epoch 191/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6856 - auc: 0.5599 - val_loss: 0.6870 - val_auc: 0.5564\n",
      "Epoch 192/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6853 - auc: 0.5609 - val_loss: 0.6871 - val_auc: 0.5557\n",
      "Epoch 193/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6855 - auc: 0.5603 - val_loss: 0.6870 - val_auc: 0.5561\n",
      "Epoch 194/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6856 - auc: 0.5599 - val_loss: 0.6870 - val_auc: 0.5559\n",
      "Epoch 195/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6855 - auc: 0.5601 - val_loss: 0.6870 - val_auc: 0.5560\n",
      "Epoch 196/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6855 - auc: 0.5607 - val_loss: 0.6870 - val_auc: 0.5564\n",
      "Epoch 197/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6857 - auc: 0.5598 - val_loss: 0.6868 - val_auc: 0.5566\n",
      "Epoch 198/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6855 - auc: 0.5600 - val_loss: 0.6869 - val_auc: 0.5564\n",
      "Epoch 199/500\n",
      "436/436 [==============================] - 32s 74ms/step - loss: 0.6855 - auc: 0.5604 - val_loss: 0.6869 - val_auc: 0.5567\n",
      "Epoch 200/500\n",
      "436/436 [==============================] - 33s 77ms/step - loss: 0.6855 - auc: 0.5600 - val_loss: 0.6870 - val_auc: 0.5564\n",
      "Epoch 201/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6855 - auc: 0.5603 - val_loss: 0.6870 - val_auc: 0.5561\n",
      "Epoch 202/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6856 - auc: 0.5597 - val_loss: 0.6869 - val_auc: 0.5569\n",
      "Epoch 203/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6855 - auc: 0.5601 - val_loss: 0.6870 - val_auc: 0.5564\n",
      "Epoch 204/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6855 - auc: 0.5601 - val_loss: 0.6869 - val_auc: 0.5564\n",
      "Epoch 205/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6856 - auc: 0.5600 - val_loss: 0.6869 - val_auc: 0.5570\n",
      "Epoch 206/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6854 - auc: 0.5604 - val_loss: 0.6870 - val_auc: 0.5565\n",
      "Epoch 207/500\n",
      "436/436 [==============================] - 33s 77ms/step - loss: 0.6853 - auc: 0.5611 - val_loss: 0.6871 - val_auc: 0.5561\n",
      "Epoch 208/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6853 - auc: 0.5609 - val_loss: 0.6870 - val_auc: 0.5564\n",
      "Epoch 209/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6854 - auc: 0.5605 - val_loss: 0.6871 - val_auc: 0.5562\n",
      "Epoch 210/500\n",
      "436/436 [==============================] - 34s 79ms/step - loss: 0.6854 - auc: 0.5608 - val_loss: 0.6870 - val_auc: 0.5561\n",
      "Epoch 211/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6853 - auc: 0.5608 - val_loss: 0.6871 - val_auc: 0.5562\n",
      "Epoch 212/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6855 - auc: 0.5607 - val_loss: 0.6870 - val_auc: 0.5562\n",
      "Epoch 213/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6853 - auc: 0.5614 - val_loss: 0.6872 - val_auc: 0.5554\n",
      "Epoch 214/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6853 - auc: 0.5602 - val_loss: 0.6870 - val_auc: 0.5561\n",
      "Epoch 215/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6854 - auc: 0.5601 - val_loss: 0.6870 - val_auc: 0.5557\n",
      "Epoch 216/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6853 - auc: 0.5610 - val_loss: 0.6871 - val_auc: 0.5557\n",
      "Epoch 217/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6853 - auc: 0.5606 - val_loss: 0.6870 - val_auc: 0.5559\n",
      "Epoch 218/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6852 - auc: 0.5613 - val_loss: 0.6870 - val_auc: 0.5563\n",
      "Epoch 219/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6853 - auc: 0.5608 - val_loss: 0.6872 - val_auc: 0.5561\n",
      "Epoch 220/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6853 - auc: 0.5608 - val_loss: 0.6869 - val_auc: 0.5564\n",
      "Epoch 221/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6851 - auc: 0.5617 - val_loss: 0.6870 - val_auc: 0.5561\n",
      "Epoch 222/500\n",
      "436/436 [==============================] - 33s 77ms/step - loss: 0.6854 - auc: 0.5604 - val_loss: 0.6870 - val_auc: 0.5564\n",
      "Epoch 223/500\n",
      "436/436 [==============================] - 34s 79ms/step - loss: 0.6852 - auc: 0.5608 - val_loss: 0.6869 - val_auc: 0.5564\n",
      "Epoch 224/500\n",
      "436/436 [==============================] - 33s 77ms/step - loss: 0.6853 - auc: 0.5613 - val_loss: 0.6870 - val_auc: 0.5559\n",
      "Epoch 225/500\n",
      "436/436 [==============================] - 34s 79ms/step - loss: 0.6852 - auc: 0.5609 - val_loss: 0.6870 - val_auc: 0.5562\n",
      "Epoch 226/500\n",
      "436/436 [==============================] - 33s 75ms/step - loss: 0.6853 - auc: 0.5609 - val_loss: 0.6871 - val_auc: 0.5560\n",
      "Epoch 227/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6854 - auc: 0.5605 - val_loss: 0.6870 - val_auc: 0.5563\n",
      "Epoch 228/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6853 - auc: 0.5608 - val_loss: 0.6872 - val_auc: 0.5560\n",
      "Epoch 229/500\n",
      "436/436 [==============================] - 33s 76ms/step - loss: 0.6852 - auc: 0.5611 - val_loss: 0.6870 - val_auc: 0.5561\n",
      "Epoch 230/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6853 - auc: 0.5607 - val_loss: 0.6872 - val_auc: 0.5558\n",
      "Epoch 231/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6852 - auc: 0.5612 - val_loss: 0.6870 - val_auc: 0.5563\n",
      "Epoch 232/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6853 - auc: 0.5606 - val_loss: 0.6870 - val_auc: 0.5562\n",
      "Epoch 233/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6852 - auc: 0.5611 - val_loss: 0.6870 - val_auc: 0.5562\n",
      "Epoch 234/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6852 - auc: 0.5614 - val_loss: 0.6870 - val_auc: 0.5562\n",
      "Epoch 235/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6851 - auc: 0.5614 - val_loss: 0.6872 - val_auc: 0.5561\n",
      "Epoch 236/500\n",
      "436/436 [==============================] - 33s 77ms/step - loss: 0.6851 - auc: 0.5619 - val_loss: 0.6871 - val_auc: 0.5557\n",
      "Epoch 237/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6851 - auc: 0.5614 - val_loss: 0.6870 - val_auc: 0.5559\n",
      "Epoch 238/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6852 - auc: 0.5610 - val_loss: 0.6871 - val_auc: 0.5560\n",
      "Epoch 239/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6850 - auc: 0.5621 - val_loss: 0.6871 - val_auc: 0.5562\n",
      "Epoch 240/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6851 - auc: 0.5612 - val_loss: 0.6871 - val_auc: 0.5557\n",
      "Epoch 241/500\n",
      "436/436 [==============================] - 34s 79ms/step - loss: 0.6851 - auc: 0.5610 - val_loss: 0.6871 - val_auc: 0.5560\n",
      "Epoch 242/500\n",
      "436/436 [==============================] - 35s 81ms/step - loss: 0.6851 - auc: 0.5615 - val_loss: 0.6872 - val_auc: 0.5557\n",
      "Epoch 243/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6850 - auc: 0.5618 - val_loss: 0.6872 - val_auc: 0.5561\n",
      "Epoch 244/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6850 - auc: 0.5622 - val_loss: 0.6870 - val_auc: 0.5560\n",
      "Epoch 245/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6851 - auc: 0.5612 - val_loss: 0.6871 - val_auc: 0.5558\n",
      "Epoch 246/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6851 - auc: 0.5614 - val_loss: 0.6869 - val_auc: 0.5565\n",
      "Epoch 247/500\n",
      "436/436 [==============================] - 34s 79ms/step - loss: 0.6849 - auc: 0.5622 - val_loss: 0.6871 - val_auc: 0.5556\n",
      "Epoch 248/500\n",
      "436/436 [==============================] - 35s 79ms/step - loss: 0.6850 - auc: 0.5620 - val_loss: 0.6871 - val_auc: 0.5560\n",
      "Epoch 249/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6851 - auc: 0.5613 - val_loss: 0.6869 - val_auc: 0.5565\n",
      "Epoch 250/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6850 - auc: 0.5618 - val_loss: 0.6870 - val_auc: 0.5565\n",
      "Epoch 251/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6850 - auc: 0.5616 - val_loss: 0.6872 - val_auc: 0.5560\n",
      "Epoch 252/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6849 - auc: 0.5619 - val_loss: 0.6871 - val_auc: 0.5563\n",
      "Epoch 253/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6851 - auc: 0.5616 - val_loss: 0.6869 - val_auc: 0.5567\n",
      "Epoch 254/500\n",
      "436/436 [==============================] - 33s 77ms/step - loss: 0.6851 - auc: 0.5615 - val_loss: 0.6870 - val_auc: 0.5568\n",
      "Epoch 255/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6851 - auc: 0.5613 - val_loss: 0.6872 - val_auc: 0.5561\n",
      "Epoch 256/500\n",
      "436/436 [==============================] - 34s 79ms/step - loss: 0.6850 - auc: 0.5617 - val_loss: 0.6872 - val_auc: 0.5560\n",
      "Epoch 257/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6851 - auc: 0.5614 - val_loss: 0.6872 - val_auc: 0.5558\n",
      "Epoch 258/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6850 - auc: 0.5617 - val_loss: 0.6871 - val_auc: 0.5558\n",
      "Epoch 259/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6850 - auc: 0.5614 - val_loss: 0.6871 - val_auc: 0.5556\n",
      "Epoch 260/500\n",
      "436/436 [==============================] - 34s 79ms/step - loss: 0.6849 - auc: 0.5619 - val_loss: 0.6870 - val_auc: 0.5561\n",
      "Epoch 261/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6850 - auc: 0.5620 - val_loss: 0.6870 - val_auc: 0.5567\n",
      "Epoch 262/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6849 - auc: 0.5621 - val_loss: 0.6872 - val_auc: 0.5563\n",
      "Epoch 263/500\n",
      "436/436 [==============================] - 33s 77ms/step - loss: 0.6848 - auc: 0.5627 - val_loss: 0.6871 - val_auc: 0.5564\n",
      "Epoch 264/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6850 - auc: 0.5614 - val_loss: 0.6871 - val_auc: 0.5559\n",
      "Epoch 265/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6850 - auc: 0.5615 - val_loss: 0.6873 - val_auc: 0.5558\n",
      "Epoch 266/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6849 - auc: 0.5617 - val_loss: 0.6872 - val_auc: 0.5556\n",
      "Epoch 267/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6850 - auc: 0.5615 - val_loss: 0.6872 - val_auc: 0.5557\n",
      "Epoch 268/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6849 - auc: 0.5622 - val_loss: 0.6871 - val_auc: 0.5565\n",
      "Epoch 269/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6849 - auc: 0.5618 - val_loss: 0.6872 - val_auc: 0.5558\n",
      "Epoch 270/500\n",
      "436/436 [==============================] - 33s 77ms/step - loss: 0.6849 - auc: 0.5622 - val_loss: 0.6872 - val_auc: 0.5559\n",
      "Epoch 271/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6847 - auc: 0.5628 - val_loss: 0.6874 - val_auc: 0.5554\n",
      "Epoch 272/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6849 - auc: 0.5623 - val_loss: 0.6872 - val_auc: 0.5557\n",
      "Epoch 273/500\n",
      "436/436 [==============================] - 35s 79ms/step - loss: 0.6847 - auc: 0.5626 - val_loss: 0.6871 - val_auc: 0.5562\n",
      "Epoch 274/500\n",
      "436/436 [==============================] - 35s 79ms/step - loss: 0.6848 - auc: 0.5623 - val_loss: 0.6872 - val_auc: 0.5558\n",
      "Epoch 275/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6849 - auc: 0.5623 - val_loss: 0.6872 - val_auc: 0.5560\n",
      "Epoch 276/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6848 - auc: 0.5624 - val_loss: 0.6871 - val_auc: 0.5556\n",
      "Epoch 277/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6848 - auc: 0.5623 - val_loss: 0.6872 - val_auc: 0.5560\n",
      "Epoch 278/500\n",
      "436/436 [==============================] - 34s 79ms/step - loss: 0.6848 - auc: 0.5616 - val_loss: 0.6873 - val_auc: 0.5559\n",
      "Epoch 279/500\n",
      "436/436 [==============================] - 34s 77ms/step - loss: 0.6849 - auc: 0.5620 - val_loss: 0.6872 - val_auc: 0.5562\n",
      "Epoch 280/500\n",
      "436/436 [==============================] - 34s 79ms/step - loss: 0.6848 - auc: 0.5629 - val_loss: 0.6872 - val_auc: 0.5559\n",
      "Epoch 281/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6848 - auc: 0.5623 - val_loss: 0.6873 - val_auc: 0.5557\n",
      "Epoch 282/500\n",
      "436/436 [==============================] - 35s 79ms/step - loss: 0.6848 - auc: 0.5620 - val_loss: 0.6872 - val_auc: 0.5560\n",
      "Epoch 283/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6849 - auc: 0.5619 - val_loss: 0.6871 - val_auc: 0.5565\n",
      "Epoch 284/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6847 - auc: 0.5629 - val_loss: 0.6873 - val_auc: 0.5560\n",
      "Epoch 285/500\n",
      "436/436 [==============================] - 34s 79ms/step - loss: 0.6848 - auc: 0.5622 - val_loss: 0.6871 - val_auc: 0.5559\n",
      "Epoch 286/500\n",
      "436/436 [==============================] - 34s 79ms/step - loss: 0.6849 - auc: 0.5614 - val_loss: 0.6870 - val_auc: 0.5564\n",
      "Epoch 287/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6849 - auc: 0.5621 - val_loss: 0.6873 - val_auc: 0.5557\n",
      "Epoch 288/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6847 - auc: 0.5626 - val_loss: 0.6871 - val_auc: 0.5564\n",
      "Epoch 289/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6847 - auc: 0.5625 - val_loss: 0.6871 - val_auc: 0.5559\n",
      "Epoch 290/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6847 - auc: 0.5626 - val_loss: 0.6872 - val_auc: 0.5561\n",
      "Epoch 291/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6848 - auc: 0.5621 - val_loss: 0.6872 - val_auc: 0.5561\n",
      "Epoch 292/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6848 - auc: 0.5622 - val_loss: 0.6872 - val_auc: 0.5559\n",
      "Epoch 293/500\n",
      "436/436 [==============================] - 35s 81ms/step - loss: 0.6848 - auc: 0.5622 - val_loss: 0.6872 - val_auc: 0.5560\n",
      "Epoch 294/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6848 - auc: 0.5624 - val_loss: 0.6870 - val_auc: 0.5563\n",
      "Epoch 295/500\n",
      "436/436 [==============================] - 35s 79ms/step - loss: 0.6847 - auc: 0.5629 - val_loss: 0.6872 - val_auc: 0.5561\n",
      "Epoch 296/500\n",
      "436/436 [==============================] - 35s 79ms/step - loss: 0.6846 - auc: 0.5627 - val_loss: 0.6872 - val_auc: 0.5557\n",
      "Epoch 297/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6848 - auc: 0.5620 - val_loss: 0.6872 - val_auc: 0.5562\n",
      "Epoch 298/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6847 - auc: 0.5628 - val_loss: 0.6872 - val_auc: 0.5564\n",
      "Epoch 299/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6848 - auc: 0.5624 - val_loss: 0.6872 - val_auc: 0.5563\n",
      "Epoch 300/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6846 - auc: 0.5631 - val_loss: 0.6872 - val_auc: 0.5561\n",
      "Epoch 301/500\n",
      "436/436 [==============================] - 35s 79ms/step - loss: 0.6847 - auc: 0.5625 - val_loss: 0.6870 - val_auc: 0.5567\n",
      "Epoch 302/500\n",
      "436/436 [==============================] - 35s 79ms/step - loss: 0.6848 - auc: 0.5618 - val_loss: 0.6873 - val_auc: 0.5560\n",
      "Epoch 303/500\n",
      "436/436 [==============================] - 35s 79ms/step - loss: 0.6846 - auc: 0.5630 - val_loss: 0.6873 - val_auc: 0.5559\n",
      "Epoch 304/500\n",
      "436/436 [==============================] - 35s 79ms/step - loss: 0.6847 - auc: 0.5629 - val_loss: 0.6874 - val_auc: 0.5555\n",
      "Epoch 305/500\n",
      "436/436 [==============================] - 35s 81ms/step - loss: 0.6846 - auc: 0.5626 - val_loss: 0.6872 - val_auc: 0.5563\n",
      "Epoch 306/500\n",
      "436/436 [==============================] - 35s 79ms/step - loss: 0.6847 - auc: 0.5621 - val_loss: 0.6871 - val_auc: 0.5563\n",
      "Epoch 307/500\n",
      "436/436 [==============================] - 35s 79ms/step - loss: 0.6848 - auc: 0.5623 - val_loss: 0.6872 - val_auc: 0.5564\n",
      "Epoch 308/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6845 - auc: 0.5632 - val_loss: 0.6872 - val_auc: 0.5562\n",
      "Epoch 309/500\n",
      "436/436 [==============================] - 35s 81ms/step - loss: 0.6847 - auc: 0.5624 - val_loss: 0.6872 - val_auc: 0.5566\n",
      "Epoch 310/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6847 - auc: 0.5626 - val_loss: 0.6871 - val_auc: 0.5563\n",
      "Epoch 311/500\n",
      "436/436 [==============================] - 35s 79ms/step - loss: 0.6845 - auc: 0.5633 - val_loss: 0.6871 - val_auc: 0.5564\n",
      "Epoch 312/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6848 - auc: 0.5622 - val_loss: 0.6872 - val_auc: 0.5566\n",
      "Epoch 313/500\n",
      "436/436 [==============================] - 35s 79ms/step - loss: 0.6846 - auc: 0.5627 - val_loss: 0.6871 - val_auc: 0.5563\n",
      "Epoch 314/500\n",
      "436/436 [==============================] - 35s 79ms/step - loss: 0.6846 - auc: 0.5628 - val_loss: 0.6871 - val_auc: 0.5563\n",
      "Epoch 315/500\n",
      "436/436 [==============================] - 34s 79ms/step - loss: 0.6846 - auc: 0.5628 - val_loss: 0.6872 - val_auc: 0.5560\n",
      "Epoch 316/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6845 - auc: 0.5631 - val_loss: 0.6871 - val_auc: 0.5565\n",
      "Epoch 317/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6847 - auc: 0.5627 - val_loss: 0.6871 - val_auc: 0.5562\n",
      "Epoch 318/500\n",
      "436/436 [==============================] - 35s 81ms/step - loss: 0.6846 - auc: 0.5630 - val_loss: 0.6871 - val_auc: 0.5567\n",
      "Epoch 319/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6845 - auc: 0.5628 - val_loss: 0.6871 - val_auc: 0.5562\n",
      "Epoch 320/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6847 - auc: 0.5629 - val_loss: 0.6870 - val_auc: 0.5564\n",
      "Epoch 321/500\n",
      "436/436 [==============================] - 34s 79ms/step - loss: 0.6845 - auc: 0.5637 - val_loss: 0.6872 - val_auc: 0.5566\n",
      "Epoch 322/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6846 - auc: 0.5627 - val_loss: 0.6871 - val_auc: 0.5564\n",
      "Epoch 323/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6845 - auc: 0.5631 - val_loss: 0.6871 - val_auc: 0.5564\n",
      "Epoch 324/500\n",
      "436/436 [==============================] - 35s 79ms/step - loss: 0.6845 - auc: 0.5632 - val_loss: 0.6871 - val_auc: 0.5566\n",
      "Epoch 325/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6845 - auc: 0.5628 - val_loss: 0.6872 - val_auc: 0.5559\n",
      "Epoch 326/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6845 - auc: 0.5632 - val_loss: 0.6871 - val_auc: 0.5565\n",
      "Epoch 327/500\n",
      "436/436 [==============================] - 35s 80ms/step - loss: 0.6848 - auc: 0.5621 - val_loss: 0.6874 - val_auc: 0.5552\n",
      "Epoch 328/500\n",
      "436/436 [==============================] - 36s 82ms/step - loss: 0.6845 - auc: 0.5630 - val_loss: 0.6872 - val_auc: 0.5557\n",
      "Epoch 329/500\n",
      "436/436 [==============================] - 35s 79ms/step - loss: 0.6845 - auc: 0.5633 - val_loss: 0.6872 - val_auc: 0.5561\n",
      "Epoch 330/500\n",
      "436/436 [==============================] - 35s 79ms/step - loss: 0.6845 - auc: 0.5634 - val_loss: 0.6871 - val_auc: 0.5566\n",
      "Epoch 331/500\n",
      "436/436 [==============================] - 34s 78ms/step - loss: 0.6846 - auc: 0.5626 - val_loss: 0.6872 - val_auc: 0.5562\n",
      "Epoch 332/500\n",
      " 24/436 [>.............................] - ETA: 31s - loss: 0.6851 - auc: 0.5608"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "SEEDS = [10, 15]\n",
    "\n",
    "for j, SEED in enumerate(SEEDS):\n",
    "    set_all_seeds(SEED)\n",
    "\n",
    "    model = create_model(X.shape[-1],len(resp_cols))\n",
    "    # X_train, X_test = X[train_indices], X[test_indices]\n",
    "    # y_train, y_test = y[train_indices], y[test_indices]\n",
    "    history = model.fit(X[train_indices],y[train_indices],validation_data=(X[test_indices],y[test_indices]),epochs=500,batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9nv-z0vavoX"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "SEEDS = [2021, 2022]\n",
    "\n",
    "for j, SEED in enumerate(SEEDS):\n",
    "    set_all_seeds(SEED)\n",
    "\n",
    "    model = create_model(X.shape[-1],len(resp_cols))\n",
    "\n",
    "    model.fit(X,y,epochs=150,batch_size=4096)\n",
    "\n",
    "    model.save_weights(f'models_v{v}/model_{SEED}.hdf5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "kaggle-JaneStreet-model-version.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
